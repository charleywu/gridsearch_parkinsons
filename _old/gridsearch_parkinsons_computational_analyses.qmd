---
title: " Exploration and Exploitation in Parkinson’s Disease: Computational Analyses"
author: 
  - name: Björn Meder
    affiliation: Health and Medical University, Potsdam, Germany
  - name: Martha Sterf
    affiliation: Medical School Berlin, Berlin, Germany
  - name: Charley M. Wu
    affiliation: University of Tübingen, Tübingen, Germany
  - name: Matthias Guggenmos
    affiliation: Health and Medical University, Potsdam, Germany
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    toc: true
    theme: zephyr
    code-fold: true
    toc-location: left-body
    classoption: fleqn
    lightbox: true
    number-sections: true
    number-figures: true
    self-contained: true
    mathjax: default
    grid:
      sidebar-width: 300px
      body-width: 1000px
      margin-width: 200px
      gutter-width: 1.5rem
    code-links:
      - text: "GitHub Repository"
        href: "https://github.com/charleywu/gridsearch_parkinsons"
  pdf:
    toc: true
    number-sections: true
    number-figures: true
  docx:
    toc: true
    number-sections: true
    number-figures: true
editor: 
  default: source
editor_options: 
  chunk_output_type: console
bibliography: gridsearch_parkinson.bib
csl: https://www.zotero.org/styles/apa
cite-method: citeproc
---


```{r, results = "hide", message=FALSE}
# Housekeeping: Load packages and helper functions
# Housekeeping
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.align='center')

options(knitr.kable.NA = '')

packages <- c('gridExtra', 'BayesFactor', 'tidyverse', "RColorBrewer", "lme4", "sjPlot", "lsr", "brms", "kableExtra", "afex", "emmeans", "viridis", "ggpubr", "hms", "scales", "cowplot", "waffle", "ggthemes", "parameters", "rstatix", "magick", "grid", "cetcolor", "ggcorrplot")

installed <- packages %in% rownames(installed.packages())
if (any(!installed)) {
  install.packages(packages[!installed])
}

# Load all packages
lapply(packages, require, character.only = TRUE)

set.seed(0815)

# file with various statistical functions, among other things it provides tests for Bayes Factors (BFs)
source('statisticalTests.R')

# Wrapper for brm models such that it saves the full model the first time it is run, otherwise it loads it from disk
run_model <- function(expr, modelName, path='brm', reuse = TRUE) {
  path <- paste0(path,'/', modelName, ".brm")
  if (reuse) {
    fit <- suppressWarnings(try(readRDS(path), silent = TRUE))
  }
  if (is(fit, "try-error")) {
    fit <- eval(expr)
    saveRDS(fit, file = path)
  }
  fit
}


# Setting some plotting params
w_box          <- 0.2      # width of boxplot, also used for jittering points and lines    
line_jitter    <- w_box / 2
xAnnotate      <- -0.3

# jitter params
jit_height  <- 0.01
jit_width   <- 0.05
jit_alpha   <- 0.6

# colors for age groups
groupcolors    <- c("#d95f02", "#1b9e77", "#7570b3")
choice3_colors <- c("#e7298a", "#66a61e", "#e6ab02")

```

# Preamble

This document provides R code for the computational analyses and plots reported in the article 

Meder, B., Sterf, M. Wu, C.M, & Guggenmos, M. (2025). Uncertainty-directed and random exploration in Parkinson’s disease. _PsyArXiv_ 

All analyses are fully reproducible, with the R code shown alongside the results, and random seeds set to ensure identical outputs across runs. Full session info is provided at the end of the document.
All materials, including this document and all data, are available at: 

[https://github.com/charleywu/gridsearch_parkinsons](https://github.com/charleywu/gridsearch_parkinsons)  


# Load data

There are two files with behavioral data: data_gridsearch_Parkinson.csv

The behavioral data are stored in

- *data_gridsearch_parkinson.csv*, which contains the behavioral data from rounds 1-9 from the task 
- *data_gridsearch_subjects.csv*, which contains participant information.   

These files are combined to data frame _dat_, which includes the following variables:

-   *id*: participant id
-   *age* is participant age in years
-   *gender*: (m)ale, (f)emale, (d)iverse
-   *x* and *y* are the sampled coordinates on the grid
-   *chosen*: are the *x* and *y* coordinates of the chosen tile
-   *z* is the reward obtained from the chosen tile, normalized to the range 0-1. Re-clicked tiles could show small variations in the observed color (i.e., underlying reward) due to normally distributed noise,$\epsilon∼N(0,1)$.
-   *z_scaled* is the observed outcome (reward), scaled in each round to a randomly drawn maximum value in the range of 70% to 90% of the highest reward value
-   *trial* is the trial number (0-25), with 0 corresponding to the initially revealed random tile, i.e. trial 1 is the first choice
-   *round* is the round number (1 through 10), with 1=practice round (not analyzed) and 10=bonus round (analyzed only for bonus round judgments)
-   *distance* is the Manhattan distance between consecutive clicks. *NA* for trial 0, the initially revealed random tile
-   *type_choice* categorizes consecutive clicks as "repeat" (clicking the same tile as in the previous round), "near" (clicking a directly neighboring tile, i.e. distance=1), and "far" (clicking a tile with distance \> 1). *NA* for trial 0, i.e., the initially revealed random tile.
-   *previous_reward* is the reward *z* obtained on the previous step. *NA* for trial 0, i.e., the initially revealed random tile.
-   *last_ldopa*: time of the last L-Dopa dose (HH:MM)
-   *next_ldopa*: scheduled time of the next L-Dopa dose (HH:MM)
-   *time_exp*: time of the experiment (HH:MM)
-   *time_since_ldopa*: time since last L-Dopa (in minutes)

File _modelFits.csv_ contains the results of the computational model simulations (GP-UCB model and lesioned variants).

```{r}
########################################################
# get behavioral data
########################################################
dat_gridsearch <- read_csv("data/data_gridsearch_Parkinson.csv", show_col_types = FALSE) %>% 
  mutate(type_choice  = factor(type_choice, levels = c("Repeat", "Near", "Far"))) 

# normalize reward and previous reward
dat_gridsearch$z = dat_gridsearch$z / 50
dat_gridsearch$previous_reward = dat_gridsearch$previous_reward / 50

########################################################
# get subject data
########################################################
dat_sample <- read_delim("data/data_gridsearch_subjects.csv", escape_double = FALSE, trim_ws = TRUE, show_col_types = FALSE) %>% 
  mutate(gender = as.factor(gender),
        group = fct_recode(group,
                       "Control" = "PNP"
                       # "PD+"     = "PD+",
                       # "PD-"     = "PD-"
                       ),
    group = fct_relevel(group, "PD-", "PD+", "Control")
  ) %>% 
  mutate(last_ldopa = if_else(group != "Control", as_hms(last_ldopa), as_hms(NA)),
         next_ldopa = if_else(group != "Control", as_hms(next_ldopa), as_hms(NA)),
         time_exp = if_else(group != "Control", as_hms(time_exp), as_hms(NA))) %>% 
  mutate(time_since_ldopa = as.numeric(time_exp - last_ldopa, unit = "mins"))


dat <- dat_sample %>% 
  left_join(dat_gridsearch, by = "id") %>% 
  arrange(group)

########################################################
# get modeling data
########################################################
modelFits <- read.csv('modelResults/modelFit.csv') # generated by dataProcessing_gridSearchParkinson.R
# length(unique(modelFits$id))

```

# Computational Analyses: Gaussian Process Upper Confidence Bound (GP-UCB) Model

The behavioral analyses showed that individuals in a dopamine-depleted state exhibit a severe deficit in balancing exploration and exploitation. By contrast, the behavior of patients on medication was markedly improved and largely resembled controls. The increased exploration in patients off medication could result from more random choice behavior, an increased emphasis on uncertainty-directed exploration, or from impaired generalization reducing the ability to use information from one option to guide choices.

To disentangle these mechanisms, we used the Gaussian Process Upper Confidence Bound (GP-UCB) model (see Methods for formal specification). The model integrates similarity-based generalization with two distinct exploration mechanisms: uncertainty-directed exploration, which seeks to reduce uncertainty about rewards, and random exploration, which adds stochastic noise without being directed towards a particular goal [@Wu_2018grid; @Wu_et_al-generalization2025]. These processes are captured by three key parameters: the generalization parameter $\lambda$, which determines how strongly rewards are generalized across options; the uncertainty bonus $\beta$, which governs the degree of uncertainty-directed exploration by determining the value given to uncertainty; and the temperature parameter $\tau$, which captures random exploration through choice variability.

In previous research using the same experimental paradigm, this model provided the best account of exploratory behavior in healthy participants [@Wu_2018grid; @Wu:2020neurogrid; @witt2024flexible; @Schulz:2019kwg; @meder2021_development_directed_undirected; @giron2023developmental]. Importantly, by decomposing exploration into generalization ($\lambda$), uncertainty-driven exploration ($\beta$), and random exploration ($\tau$), the model allows us to identify which mechanisms are altered by PD and medication. This approach builds on prior findings that levodopa impairs discrimination learning while sparing generalization in PD [@shohamy2006dopa], that levodopa reduces directed exploration in healthy participants [@chakroun2020dopaminergic], and that PD disrupts the overall exploration-exploitation balance [@gilmour2024impaired; @seymour2016deepbrainstimulationPD; @djamshidian2011novelty].


The GP-UCB model comprises three components: a learning model, which uses Bayesian inference to generate predictions about the rewards associated with each option (tiles); a sampling strategy, which uses both the reward expectations and the uncertainty about this expectation to evaluate options; and a choice rule, which maps the valuation of options onto choice probabilities (see Methods for formal specification).

## Learning model: Gaussian Process (GP) generalization

The similarity-based learning mechanism is implemented by a Gaussian Process (GP) [@williams2006gaussian;@schulz2018tutorialGP], which learns an unknown spatial value function from noisy observations (e.g., a mapping from spatial location to expected reward). The amount of generalization depends on the similarity of options, where similarity is defined as spatial proximity: options that are closer to each other are assumed to be more alike (i.e., yield similar rewards) than options that are further away. The degree to which learning about one location influences reward expectations of other locations is governed by the parameter $\lambda$, indicating how strongly a learner extrapolates from known rewards to other locations. Higher values of $\lambda$ imply stronger generalization, whereas lower values correspond to weaker generalization.

Formally, a GP defines a probability distribution over functions mapping inputs to outputs $f: \mathcal{X} \rightarrow Y$. In our case, these functions map grid locations $\mathbf{x}\in \mathcal{X}$ to scalar reward observations $y \in Y$, with the prior distribution taking the form of a multivariate Gaussian:

$$  
f \sim \mathcal{GP}\big(m(\mathbf{x}),\, k(\mathbf{x}, \mathbf{x}')\big).
$$  {#eq-GP_prior}

The GP is fully specified by prior mean function $m(\mathbf{x})$ defining the prior expectations of each input, and a kernel (covariance) $k(\mathbf{x}, \mathbf{x}')$ encoding how strongly rewards at two locations are expected to covary as a function of their distance (@eq-RBF). Without loss of generality, we set the prior mean to zero [@williams2006gaussian] and use the common radial basis function (RBF) kernel: 


$$ 
k(\mathbf{x}, \mathbf{x}') = \exp\left(-\frac{||\mathbf{x}-\mathbf{x}'||^2}{2\lambda^2}\right).
$$ {#eq-RBF}

Here, $\mathbf{x}$ and $\mathbf{x}'$ are the coordinates of two tiles on the grid, and $\lambda$ is the length-scale parameter, which governs the amount of generalization (i.e., the smoothness of the function). Higher values of $\lambda$ imply smoother functions, leading to stronger expectations regarding reward correlations. Lower values of $\lambda$ entail rougher functions, i.e. less correlation among similar options. %As $\lambda \to \infty$, the RBF kernel assumes functions approaching linearity; as $\lambda \to 0$, there ceases to be any spatial correlation, meaning that learning of options' rewards happens independently. In our analyses, we treat $\lambda$ as a free parameter representing the extent to which learners generalize rewards as function of spatial proximity.

To compute posterior predictions for any target location $\mathbf{x}_\star$, we condition the model on a set of observations $\mathcal{D}_t=\{X_{t}, \textbf{y}_{t}\}$  of choices $X_{t} = [\mathbf{x}_1, \ldots \mathbf{x}_t]$ and corresponding reward observations $\mathbf{y}_t = [y_1, \ldots, y_t]$ at time $t$. This posterior also takes the form of a multivariate Gaussian:

$$
f(\mathbf{x}_\star) \mid \mathcal{D}_t \sim \mathcal{N}(m(\mathbf{x}_\star|\mathcal{D}_t), v(\mathbf{x}_\star|\mathcal{D}_t)),
$$ {#eq-GP_post}

which is entirely defined by a posterior mean

$$
m(\mathbf{x}_\star|\mathcal{D}_t)
$$  {#eq-GP_post_mean}

and a posterior variance 

$$
v(\mathbf{x}_\star|\mathcal{D}_t)
$$ {#eq-GP_post_var}. 

These are computed as:

$$
m(\mathbf{x}_\star|\mathcal{D}_t) = \mathbf{k}_\star \big[ K_{X,X} + \sigma_\epsilon^2 I \big]^{-1} \mathbf{y}_t
$$ {#eq-updating-mean}

$$
\sigma^2(\mathbf{x}_\star|\mathcal{D}_t) = k(\mathbf{x}_\star,\mathbf{x}_\star) -  \mathbf{k}_\star^\top \big[  K_{X,X} + \sigma_\epsilon^2 I \big]^{-1}  \mathbf{k}_\star
$$ {#eq-updating-var}

Here, $\mathbf{k}_\star=[k(\mathbf{x}_1,\mathbf{x_\star}), \ldots, k(\mathbf{x}_t,\mathbf{x_\star})]$ is the vector of kernel similarities between past observations and the target location, $K_{X,X}$ is a matrix of pairwise kernel similarities between all past observations in $X_t$, $I$ is a $t \times t$ identity matrix, and $\sigma_\epsilon^2$ is the observation noise capturing the stochasticity of reward observations and is fixed to the true reward variability of each arm of the bandit  $\sigma_\epsilon^2=.0001$.

## Sampling strategy: Balancing rewards and uncertainty through Upper Confidence Bound sampling
Options are valued according to Upper Confidence Bound (UCB) sampling, which considers both reward expectations and the associated uncertainty [@auer2002using]. UCB sampling implements a form of uncertainty-directed exploration that balances exploiting high rewards and seeking information. Uncertainty is valued positively to promote exploration of underexplored options, with the strength of this uncertainty bonus represented by the parameter $\beta$. 

$$
\text{UCB}(\mathbf{x})= m(\mathbf{x}|\mathcal{D}_t)+\beta\sqrt{v(\mathbf{x}_\star|\mathcal{D}_t)}
$$ {#eq-UCB}

where the expected reward of an option $m(\mathbf{x}|\mathcal{D}_t)$ captures its exploitation value, and the scaled uncertainty$\beta\sqrt{v(\mathbf{x}_\star|\mathcal{D}_t)}$ captures its exploration value, with  $\beta$ modulating how much exploration is promoted relative to exploitation. Higher values of $\beta$ reflect a stronger drive to explore uncertain options, while lower values reflect a preference for exploiting known high-reward options.

## Choice rule: Translating value into action
After computing UCB values for each option, the model does not always pick the most valuable one. Instead, it samples probabilistically using a softmax choice function, which adds random decision noise to the choice process:

This is implemented using a softmax function: 

$$
p(\mathbf{x}) = \frac{\exp(\text{UCB}(\mathbf{x})/\tau)}{\sum_{j=1}^{N}\exp(\text{UCB}(\mathbf{x}_j)/\tau)}.
$$ {#eq-softmax}

The amount of randomness in the choice probabilities is governed by the _temperature parameter_ $\tau$. Higher values of $\tau$ make the choice probabilities more uniform, such that the choice behavior is less influenced by options' UCB values and more random. Lower value of $\tau$ imply that the learner is more sensitive to options' UCB values, making them increasingly likely to be selected. In the limits, if $\tau \rightarrow 0$, choice behavior reduces to a greedy mean policy that always selects the option with the highest value (pure exploitation), and if $\tau \rightarrow \infty$ all options are chose with equal probability (pure exploration).
Here, we treat the temperature parameter $\tau$ as a computational marker of a learner’s tendency to explore randomly, i.e., in an undirected fashion through inherent decision noise. Higher values of $\tau$ correspond to more random exploration, and lower values to more deterministic choice behavior.


### GP-UCB model parameters
Associated with each model component is a free parameter that we estimate through out-of-sample cross validation. These parameters provide a window into distinct aspects of learning and exploration: 

1. The length-scale parameter $\lambda$ of the RBF kernel (@eq-RBF) captures how strongly a participant generalizes based on the observed evidence, i.e., the rewards obtained from previous choices.
2. The uncertainty bonus $\beta$ in the UCB valuation of options (@eq-UCB) represents to the level of directed exploration, i.e., how much expected rewards are inflated through an "uncertainty bonus".
3. The temperature parameter $\tau$ of the softmax choice rule (@eq-softmax) corresponds to the amount of sampling noise, i.e., extent of random exploration. 

# Model comparison
We tested the GP-UCB model in its ability to model learning and predicting each participants' search and decision-making behavior. To assess the contribution of each component of the model (generalization, uncertainty-directed exploration, and random exploration) we compare the predictive accuracy of the GP-UCB model to model variants where we lesion away each component.

## Lesioned models

To establish that all components of the GP-UCB model are required to explain behavior, we implemented three lesion variants of the model [@giron2023developmental].

###  $\lambda$ lesion model
The $\lambda$ lesion model removes the ability to generalize, such that options’ rewards are learned independently via a Bayesian Mean Tracker (BMT). The BMT is a Kalman filter with time-invariant rewards [@dayan2000learning; @wu2022time], and as such can be interpreted as a Bayesian variant [@gershman2015unifying] of the classic Rescorla-Wagner [@rescorla1972theory] or Q-learning models [@watkins1992q]. Intuitively, reward estimates are updated as a function of prediction error, where the learning rate is dynamically defined based on the degree of uncertainty of the model.

Like the GP, the BMT also assumes a Gaussian prior distribution of reward expectations, but does so independently for each option $\mathbf{x}$:

$$
p \big(r_0(\mathbf{x})\big) \sim \mathcal{N}\big(m_0(\mathbf{x}),v_0(\mathbf{x})\big)
$$ {#eq-BMTprior}

where $m_0(\mathbf{x})=0$ as in the GP, and we set $v_0(\mathbf{x})=5$ following [@giron2023developmental].

The BMT then computes a posterior distribution of the expected reward for each option, also in the form of a Gaussian, but where the posterior mean $m_t(\mathbf{x})$ and posterior variance $v_t(\mathbf{x})$ are defined independently for each option and computed by the following updates:

$$
m_{t+1}(\mathbf{x}) = m_t(\mathbf{x})+\delta_t(\mathbf{x})G_t(\mathbf{x})\big(y_t(\mathbf{x})-m_t(\mathbf{x})\big)
$$ {#eq-bmtMean}

$$
v_{t+1}(\mathbf{x}) = v_{t}(\mathbf{x})\big(1-\delta_t(\mathbf{x})G_t(\mathbf{x})\big)
$$ {#eq-bmtVar}

Both updates use $\delta_t(\mathbf{x})=1$ if option $\mathbf{x}$ was chosen on trial $t$, and $\delta_t(\mathbf{x})=0$ otherwise. Thus, the posterior mean and variance are only updated for the chosen option. The update of the mean is based on the prediction error $y_t(\mathbf{x})-m_t(\mathbf{x})$ between observed and anticipated reward, while the magnitude of the update is based on the Kalman gain $G_t(\mathbf{x})$:

$$
G_t(\mathbf{x})=\frac{v_{t}(\mathbf{x})}{v_{t}(\mathbf{x})+\theta_\epsilon^2}
$$ {#eq-bmtKalmangain}

analogous to the learning rate of the Rescorla-Wagner or Q-learning models. Here, the Kalman gain is dynamically defined as a ratio of variance terms, where $v_{t}(\mathbf{x})$ is the posterior variance estimate and $\theta_\epsilon^2$ is the error variance, which we treat as a free parameter and can be interpreted as an inverse sensitivity parameter. Smaller values of $\theta_\epsilon^2$ thus result in larger updates of the mean.

### $\beta$ lesion model

The $\beta$ lesion model evaluates options solely based on their expected rewards, corresponding to a mean-greedy (MG) sampling strategy, and is implemented by setting the uncertainty bonus to $\beta = 0$ (see @eq-UCB). Effectively, this equates the value of options with their posterior mean $\text{MG}(\mathbf{x}) = m(\mathbf{x}|\mathcal{D}_t)$.

###  $\tau$ lesion model
The $\tau$ lesion model replaces the softmax choice function (see @eq-softmax) with an $\epsilon$-greedy policy as an alternative mechanism for random exploration. Under this policy, with probability $\epsilon$ a random option is selected and with probability $1-\epsilon$, the option with the highest UCB value is chosen:

$$
p(\mathbf{x}) =
\begin{cases}
\text{arg max}\,\text{UCB}(\mathbf{x}), & \text{with probability } 1-\epsilon \\
\text{random option}, & \text{with probability } \epsilon
\end{cases}
$$ {#eq-epsilon}

with the parameter $\epsilon$ estimated individually for each participant.

## Model cross validation
Models' predictive accuracy was assessed using leave-one-round-out cross-validation based on maximum likelihood estimation \cite{mullen2011DEoptim}, with parameter bounds set to the range $[\exp(-5), \exp(4)]$. Specifically, we iteratively held out one round from the task, fitted each model to the remaining seven rounds, and then tested its ability to predict participants’ choices on the 25 trials of the holdout round. Predictive accuracy was quantified as the sum of negative log-likelihoods across all out-of-sample predictions. Individual parameter estimates for participants are based on averaging over the cross-validated maximum likelihood estimates.

The negative log-likelihoods served as the model evidence for the hierarchical Bayesian model selection based on protected exceedance probabilities [@rigoux2014pxp], and for quantifying predictive accuracy using a pseudo-$R^2$ measure, where the summed log loss of each model is compared to a random baseline model. Accordingly, $R^2=0$ corresponds to chance performance and $R^2=1$ corresponds to theoretically perfect predictions: 

$$
R^2 = 1 - \frac{\log \mathcal{L}(M_k)}{\log\mathcal{L}(M_{rand})}    
$${eq-R2}

Participant classification was based on which model had the highest $R^2$ (or, equivalently, lowest log loss). We additionally performed a model comparison on the group level using the $R^2$ measure. Consistent with the hierarchical Bayesian model selection and participant classification, the GP-UCB model achieved the highest $R^2$ in each group (SI).

## Bayesian hierarchical model selection (pxp)
For group-level model selection we computed protected exceedance probabilities (pxp), which quantify the probability that a given model is more frequent in the population than all competing models [@rigoux2014pxp]. In each group, the GP-UCB model outperformed all other models (@fig-model-comparison-pxp).

```{r}
#| fig-cap: "Hierarchical Bayesian model selection, where pxp defines the probability of each model being the most frequent in the population"
#| label: fig-model-comparison-pxp
#| fig-width: 10

# load data with pxp values (pre computed via Python code in github repo)
df_pxp <- read_csv("modelResults/pxp.csv")

df_pxp_long <- 
  df_pxp %>%
    pivot_longer(cols=-condition, values_to = "pxp", names_to = "model") %>% 
  mutate(
    model = factor(model,
                   levels = c("RBF_UCB", "BMT_UCB", "RBF_GM", "RBF_epsilonGreedy")),
    model = fct_recode(model,
                       "GP\nUCB" = "RBF_UCB",
                       "lambda\nlesion" = "BMT_UCB",
                       "beta\nlesion"  = "RBF_GM",
                       "tau\nlesion" = "RBF_epsilonGreedy")
  ) %>% 
  mutate(
    condition = factor(condition,
                   levels = c("PD-", "PD+", "control")),
    condition = fct_recode(condition, "Control" = "control"))

# grouped by model
p_pxp <- 
  ggplot(df_pxp_long, aes(x = condition, y = pxp, fill = condition)) +
  facet_wrap(
    ~ model,
    nrow = 1,
    labeller = as_labeller(
      c(
        "GP\nUCB"    = "GP-UCB",
        "lambda\nlesion" = "lambda~' lesion'",
        "beta\nlesion"   = "beta~' lesion'",
        "tau\nlesion"    = "tau~' lesion'"
      ),
      label_parsed
    )
  ) +
  geom_bar(stat= "identity", color = "black") +
  scale_y_continuous(name = "pxp", breaks = c(0, 0.5, 1), limits = c(0,1), expand = c(0,0),)+  
  scale_x_discrete("") +
  scale_fill_manual(values = groupcolors) + 
  ggtitle("Hierarchical Bayesian model selection") + 
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=18),
        legend.position = "none",
        legend.justification = c(0, 1),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 14),
        plot.title = element_text(size = 24)
  )

# grouped by patient group
 p_pxp <- 
ggplot(df_pxp_long, aes(x = model, y = pxp, fill = condition)) +
  facet_wrap(~ condition, nrow = 1) +
  geom_bar(stat= "identity", color = "black") +
  scale_y_continuous(
    name = "pxp",
    breaks = c(0, 0.5, 1),
    limits = c(0, 1),
    expand = c(0, 0)
  ) +
 scale_x_discrete("",
                   labels = c(
      "lambda\nlesion" = "λ\nlesion",
    "beta\nlesion"   = "β\nlesion",
    "tau\nlesion"    = "τ\nlesion"
    )) + 
  # ggthemes::scale_fill_tableau(name = NULL) +
   scale_fill_manual(values = groupcolors) + 
  ggtitle("Hierarchical Bayesian model selection") + 
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=18),
        legend.position = "none",
        legend.justification = c(0, 1),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 14),
        plot.title = element_text(size = 24)
  )

ggsave("plots/p_model_comparison_pxp.png", p_pxp, height = 4, width=8)
```

Models' predictive accuracy was assessed using a pseudo-$R^2$ measure, based on the sum of negative log-likelihoods across all out-of-sample predictions. The summed log loss is compared to a random model, such that $R^2=0$ corresponds to chance performance and $R^2=1$ corresponds to theoretically perfect predictions. 

$$
R^2 = 1 - \frac{\log \mathcal{L}(M_k)}{\log\mathcal{L}(M_{rand})},
$$
In each group, the GP-UCB model had the highest predictive accuracy (@sec-SI). 


## Model-based classification of participants

```{r}

# get subject information
groupDF <-dat_sample %>%
  select(id, age, gender,group,BDI,MMSE,hoehn_yahr,last_ldopa,next_ldopa,time_exp,time_since_ldopa) %>%
  group_by(id) %>%
  slice_head(n = 1) %>%
  arrange(group)
  
# groupDF <- dat %>% 
#   group_by(id) %>%
#   slice(1) %>%
#   ungroup()

# length(unique(dat$id))
# length(unique(groupDF$id))

modelFits <- merge(modelFits, groupDF[,c('id', 'group')], by = "id") # merge to add group 

# write individual model fits by group
# write_csv(modelFits, "modelResults/modelFits_group.csv")
# write_csv(subset(modelFits, group == "Control"), "modelResults/modelFits_control.csv")
# write_csv(subset(modelFits, group == "PD-"),"modelResults/modelFits_PD_minus.csv")
# write_csv(subset(modelFits, group == "PD+"), "modelResults/modelFits_PD_plus.csv")

# kernels <- c("RBF", "BMT") # RBF = Radial Basis Function kernel, BMT= Bayesian Mean Tracker
# acqFuncs <- c("GM", "UCB", "EG") # UCB = Upper Confidence Bound, GM=greedyMean, EG = epsilonGreedy
modelFits <-  modelFits %>%
  mutate(kernel=factor(kernel, levels=c('RBF', 'BMT'), labels=c('GP', 'BMT'))) %>%
  mutate(acq=factor(acq, levels=c('UCB', 'GM','epsilonGreedy'), labels=c('UCB', 'meanGreedy', 'epsilonGreedy')))

modelFits$ModelName = paste(modelFits$kernel, modelFits$acq, sep="-")

# Only include key comparisons
modelFits <- subset(modelFits, ModelName %in% c("GP-UCB", "BMT-UCB", "GP-meanGreedy", "GP-epsilonGreedy" ))
modelFits$ModelName = factor(modelFits$ModelName, levels = c('GP-UCB', 'BMT-UCB', 'GP-meanGreedy', 'GP-epsilonGreedy'))

#Two line name for models
modelFits$shortname <- factor(modelFits$ModelName, labels = c('GP\nUCB', 'lambda\nlesion', 'beta\nlesion', 'tau\nlesion'))
levels(modelFits$shortname) <- c('GP\nUCB', 'lambda\nlesion', 'beta\nlesion', 'tau\nlesion')

```

```{r}
# classify participants according to model R^2
df_participant_classification <- modelFits %>%
  group_by(id) %>%
  slice_max(order_by = R2, n = 1) %>%
  select(id, group, ModelName, shortname, R2) %>% 
  ungroup() %>% 
  rename(best_ModelName = ModelName,
         best_shortname = shortname,
         best_R2 = R2)

df_counts <- df_participant_classification %>%
  count(group, best_shortname)

df_percent <- df_counts %>%
  group_by(group) %>%
  mutate(
    total_in_group = sum(n),
    percent = round((n / total_in_group) * 100, 1)
  ) %>%
  ungroup()

# add most predictive model for each subject to df modelFits
modelFits <- modelFits %>% 
  left_join(df_participant_classification, by = c("id", "group"))


```

We classified participants based on which model achieved the highest cross-validated predictive accuracy (highest $R^2$; @fig-participant-classification). In each patient group, the GP-UCB model was the most predictive model for the majority of participants (Control: `r df_percent %>% filter(group == "Control", best_shortname == "GP\nUCB") %>% pull(percent)`%, PD+: `r df_percent %>% filter(group == "PD+", best_shortname == "GP\nUCB") %>% pull(percent)`%, PD-: `r df_percent %>% filter(group == "PD-", best_shortname == "GP\nUCB") %>% pull(percent)`%).

In total, out of `r sum(df_counts$n)` participants, `r sum(df_counts$n[df_counts$best_shortname == "GP\nUCB"])` (`r round(sum(df_counts$n[df_counts$best_shortname == "GP\nUCB"]) / sum(df_counts$n),3)*100`%) were best described by the GP-UCB model, `r sum(df_counts$n[df_counts$best_shortname == "lambda\nlesion"])` (`r round(sum(df_counts$n[df_counts$best_shortname == "lambda\nlesion"]) / sum(df_counts$n),3)*100`%) by the lambda lesion model, `r sum(df_counts$n[df_counts$best_shortname == "beta\nlesion"])`  (`r round(sum(df_counts$n[df_counts$best_shortname == "beta\nlesion"]) / sum(df_counts$n),3)*100`%) by the beta lesion model, and `r sum(df_counts$n[df_counts$best_shortname == "tau\nlesion"])` (`r round(sum(df_counts$n[df_counts$best_shortname == "tau\nlesion"]) / sum(df_counts$n),3)*100`%) by the tau lesion model. The results suggest that all three components of the GP-UCB model are relevant for predicting participants' behavior.

```{r}
#| fig-cap: "Classification of participants by models' cross-validated predictive accuracy. Each square is one participant, coloured according to the model that best predicted their perfomance (=highest R2)."
#| label: fig-participant-classification
#| fig-width: 12


# waffle plot
p_classification_participants <- 
  ggplot(
  data = df_counts, 
  aes(fill=best_shortname, values=n)
) +
  geom_waffle(
    color = "white", 
    size = 1, 
    n_rows = 5
  ) +
  facet_wrap(~group, nrow=1) +
  scale_x_discrete(
    expand = c(0,0,0,0)
  ) +
  scale_y_discrete(
    expand = c(0,0,0,0)
  ) +
  # ggthemes::scale_fill_tableau(name=NULL) +
  ggthemes::scale_fill_tableau(
    name = NULL,
    labels = c(
      "GP\nUCB"    = "GP-UCB",
      "lambda\nlesion" = expression(lambda ~ " lesion"),
      "beta\nlesion"   = expression(beta ~ " lesion"),
      "tau\nlesion"    = expression(tau ~ " lesion")
    )
  ) +
 # coord_equal() +
  ggtitle ("     Participant classification") +
theme_classic() +
  theme(
    legend.title = element_blank(),
    plot.title = element_text(size = 24),
    legend.position = 'bottom',
    strip.text = element_text(color = "black", size=18),
    legend.text =  element_text(colour="black", size=14),
    text = element_text(colour = "black"),
    strip.background =element_blank(),
    axis.text= element_text(colour="black", size = 18),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.spacing = unit(3, "lines"),
    legend.key.spacing.y = unit(0.4, "cm")
    # plot.margin = margin(-70, 0, -30, 0) # negativ bottom margin, otherwise artfecat when putting later together with cowplot
    )

ggsave("plots/participant_classification.png", p_classification_participants, width = 12, height = 5, dpi=300)

```


# Analysis of parameter estimates

```{r}
#| label: tbl-GP-UCB-parameters
#| tbl-cap: "Mean  (95% CI) and median parameter estimates of the GP-UCB model by group."


df_gpucb_params <- modelFits %>% filter(kernel=='GP' & acq == 'UCB') %>% 
  pivot_longer(c('lambda', 'beta', 'tau'), names_to = 'param', values_to = 'estimate') %>% 
  mutate(param = factor(param, levels = c('lambda', 'beta', 'tau'))) %>% 
  mutate(estimate_log10 = log10(estimate))


df_gpucb_params %>%
  group_by(group, param) %>%
  summarise(
    mean = mean(estimate, na.rm = TRUE),
    median = median(estimate, na.rm = TRUE),
    se = sd(estimate, na.rm = TRUE) / sqrt(n()),
    ci_lower = mean - 1.96 * se,
    ci_upper = mean + 1.96 * se,
    .groups = "drop"
  ) %>%
  mutate(
    summary = sprintf("M = %.2f [%.2f, %.2f], Mdn = %.2f", mean, ci_lower, ci_upper, median)
  ) %>%
  select(group, param, summary) %>%
  pivot_wider(names_from = param, values_from = summary) %>%
  kable(caption = "Mean (95% CI) and median parameter estimates by group", format = "html") %>% 
  kable_styling("striped", full_width = FALSE)
```


```{r}
#| fig-cap: "Parameter estimates of GP-UCB model, estimated through leave-one-round-out cross validation. Each dot is one participant."
#| label: fig-GP-CUB_params
#| fig-width: 12


# separate plots
# generalization lambda
p_gpucb_params_lambda <- 
  ggplot(subset(df_gpucb_params, param == "lambda"), aes(x = group, y = estimate, fill = group, shape = group, color = group)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "darkred") + # true lambda
  scale_y_continuous(name = "Estimate", breaks = c(0, 0.5, 1, 2), labels = c("0", "0.5", "1", "2"), limits = c(0, 1.45)) +
  geom_boxplot(alpha = 0.2, width = 0.4, outlier.shape = NA) +  
  geom_jitter(width = 0.1, size = 1.5, alpha = 0.6) +  
  stat_summary(fun = mean, geom = "point", shape = 23, fill = "white", size = 4) +
  scale_color_manual(values = groupcolors) +
  scale_fill_manual(values = groupcolors) +
  scale_x_discrete("") + 
  # ggtitle("GP-UCB parameter estimates: Group differences") + 
  ggtitle(expression("Generalization " * lambda)) + 
  theme_classic() +
  theme(legend.position = "none",
        legend.justification = c(0, 1),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 14),
        plot.title = element_text(size = 18, hjust = 0.5),
        plot.margin = margin(10, 0, 10, 0) 
  )

# p_gpucb_params_lambda  
# ggsave("plots/GP-p_gpucb_params_lambda.png", p_gpucb_params_lambda, dpi=300, width = 4, height = 4)

# exploration bonus beta
p_gpucb_params_beta <- 
  ggplot(subset(df_gpucb_params, param == "beta"), aes(x = group, y = estimate, fill = group, shape = group, color = group)) +
  scale_y_log10(name = " ", breaks = c(0.01, 0.1, 1, 10), labels = c("0.01", "0.1", "1", "10"), limits = c(0.005, 55)) +
  geom_boxplot(alpha = 0.2, width = 0.4, outlier.shape = NA) +  
  geom_jitter(width = 0.1, size = 1.5, alpha = 0.6) +  
  stat_summary(fun = mean, geom = "point", shape = 23, fill = "white", size = 4) +
  scale_color_manual(values = groupcolors) +
  scale_fill_manual(values = groupcolors) +
  scale_x_discrete("") + 
  # ggtitle("GP-UCB parameter estimates: Group differences") + 
  ggtitle(expression("Exploration bonus " * beta)) + 
  theme_classic() +
  theme(legend.position = "none",
        legend.justification = c(0, 1),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 14),
        plot.title = element_text(size = 18, hjust = 0.5),
        plot.margin = margin(10, 0, 10, 0) 
  )

# p_gpucb_params_beta  
# ggsave("plots/GP-p_gpucb_params_beta.png", p_gpucb_params_beta, dpi=300, width = 4, height = 4)

# random exploration temperature tau 
p_gpucb_params_tau <- 
  ggplot(subset(df_gpucb_params, param == "tau"), aes(x = group, y = estimate, fill = group, shape = group, color = group)) +
  scale_y_log10(name = " ", breaks = c(0.01, 0.1, 1, 10), labels = c("0.01", "0.1", "1", "10"), limits = c(0.005, 55)) +
  geom_boxplot(alpha = 0.2, width = 0.4, outlier.shape = NA) +  
  geom_jitter(width = 0.1, size = 1.5, alpha = 0.6) +  
  stat_summary(fun = mean, geom = "point", shape = 23, fill = "white", size = 4) +
  scale_color_manual(values = groupcolors) +
  scale_fill_manual(values = groupcolors) +
  scale_x_discrete("") + 
  # ggtitle("GP-UCB parameter estimates: Group differences") + 
  ggtitle(expression("Random exploration " * tau)) + 
  theme_classic() +
  theme(legend.position = "none",
        legend.justification = c(0, 1),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 14),
        plot.title = element_text(size = 18, hjust = 0.5),
        plot.margin = margin(10, 0, 10, 0) 
  )

# p_gpucb_params_tau  
# ggsave("plots/GP-p_gpucb_params_tau.png", p_gpucb_params_tau, dpi=300, width = 4, height = 4)

# out together and add title
p_gpucb_params <- grid.arrange(
  grobs = list(p_gpucb_params_lambda, p_gpucb_params_beta, p_gpucb_params_tau),
  nrow = 1,
  top = textGrob(
    "         GP-UCB parameters: Group differences", 
    x = 0,            
    hjust = 0,        
    gp = gpar(fontsize = 24)
  ),
  padding = unit(0.5, "lines") 
)

ggsave("plots/gpucb_params.png", p_gpucb_params, dpi = 300, width = 12, height = 4)
 


```

```{r}

# plot for Computational Psychiatry Conference (CPP; Tübingen, July 2025)
# 
# # Define your comparisons
# comparisons <- list(c("PD+", "PD-"), c("Control", "PD+"), c("Control", "PD-"))
# 
# # Extract function for p and BF from ttestPretty output
# # TO DO: Cumbersome via 
# extract_p_and_bf <- function(tt_string) {
#   matches <- stringr::str_match_all(tt_string, "\\$p=([^$]+)\\$|\\$BF=([^$]+)\\$")
#   flat <- unlist(matches)
#   
#   raw_vals <- flat[!is.na(flat) & grepl("^\\.?\\d+", flat)]
#   
#   # Convert to numeric and round to 2 decimal places
#   nums <- signif(as.numeric(raw_vals), 2)
#   
#   # Format with 2 decimal digits (or scientific if very small/large)
#   p_fmt <- formatC(nums[1], digits = 2, format = "f")
#   bf_fmt <- formatC(nums[2], digits = 2, format = "f")
#   
#   paste0("p=", p_fmt, ", BF=", bf_fmt)
# }
# 
# # Loop over each param and each comparison
# comparisons_df <- df_gpucb_params %>%
#   group_by(param) %>%
#   group_modify(~{
#     comparisons <- list(
#       c("Control", "PD+"),
#       c("Control", "PD-"),
#       c("PD+", "PD-")
#     )
#     
#     # For each pairwise group comparison
#     res <- purrr::map_dfr(comparisons, function(groups) {
#       g1 <- groups[1]
#       g2 <- groups[2]
#       
#       x1 <- .x$estimate[.x$group == g1]
#       x2 <- .x$estimate[.x$group == g2]
#       
#       if (length(x1) < 2 || length(x2) < 2) {
#         return(tibble(
#           group1 = g1,
#           group2 = g2,
#           p = NA_real_,
#           BF = NA_real_,
#           y.position = NA_real_
#         ))
#       }
#       
#       # Frequentist test
#       t_res <- t.test(x1, x2, paired = FALSE, var.equal = TRUE)
#       p_val <- t_res$p.value
#       
#       # Bayes Factor
#       bf <- BayesFactor::ttestBF(x = x1, y = x2, paired = FALSE)
#       bf_val <- as.numeric(BayesFactor::extractBF(bf)$bf)
#       
#       # y-position (max value in current param group × offset)
#       y_max <- max(.x$estimate, na.rm = TRUE)
#       y_pos <- y_max * runif(1, 1.05, 1.15)
#       
#       tibble(
#         group1 = g1,
#         group2 = g2,
#         p = p_val,
#         BF = bf_val,
#         y.position = y_pos
#       )
#     })
#     
#     res
#   }) %>%
#   ungroup() %>%
#   mutate(
#     p.format = case_when(
#       p < 0.001 ~ "p<.001",
#       is.na(p) ~ NA_character_,
#       TRUE ~ paste0("p=", signif(p, 2))
#     ),
#     BF.format = case_when(
#       is.na(BF) ~ NA_character_,
#       BF > 100 ~ "BF>100",
#       TRUE ~ paste0("BF=", signif(BF, 2))
#     ),
#     plot_label = paste0(p.format, ", ", BF.format)
#   )
# 
# comparisons_df$y.position <- rep(c(1.8, 2.6, 2.2), 3)
# 
# p_GP_UCB_params_CPP <-  
#   ggboxplot(df_gpucb_params, 
#           x = "group", 
#           y = "estimate",
#           color = "group", palette =groupcolors, fill = "group", alpha = 0.2,
#           add = "jitter", jitter.size = 0.5, shape = "group", title = "GP-UCB parameter estimates") +
#   facet_wrap(~param, nrow = 1) +
#   scale_y_log10(breaks = c(0.01, 0.1, 1, 10, 100), labels = c("0.01", "0.1", "1", "10", "100"), expand = expansion(mult = c(0.1, 0.15))  ) +
#   # scale_y_log10( expand = expansion(mult = c(0, 0.1))  ) +
#   # coord_cartesian(ylim = c(0.01,260)) +
#   ylab("Estimate (log scale)") +
#   xlab("") +
#     # ignore p values because of log; only done to position brackets correctly
#  # stat_compare_means(comparisons = list( c("Control", "PD+"), c("PD+", "PD-"), c("Control", "PD-")  ),
#  #                     paired = F,
#  #                     method = "t.test",
#  #                     # label = "p.format",
#  #                     aes(label = paste0("p = ", after_stat(p.format)))
#  #                     #aes(label = paste0(" "))
#  #  ) +
#     stat_pvalue_manual(
#     filter(comparisons_df, param == "lambda"),
#     label = "plot_label",
#     tip.length = 0.01, bracket.size = 0.3, size = 3
#   ) +
#     stat_pvalue_manual(
#     filter(comparisons_df, param == "beta"),
#     label = "plot_label",
#     tip.length = 0.01, bracket.size = 0.3, size = 3
#   ) +
#     stat_pvalue_manual(
#     filter(comparisons_df, param == "tau"),
#     label = "plot_label",
#     tip.length = 0.01, bracket.size = 0.3, size = 3
#   ) +
#   stat_summary(fun = mean, geom="point", shape = 23, fill = "white", size=2) +
#   theme_classic() +
#   theme(strip.background = element_blank(),  
#         strip.text = element_text(color = "black", size=14),
#         legend.position = "none",
#         plot.title = element_text(size = 20),
#         axis.text= element_text(colour="black", size = 12),
#         axis.title= element_text(colour="black", size = 12),
#         panel.spacing = unit(3, "lines") 
#   )
# 
# ggsave("plots/GP-UCB_params_CPP.png", p_GP_UCB_params_CPP, dpi = 300, width = 8, height = 5)
# ggsave("plots/GP-UCB_params_CPP.pdf", p_GP_UCB_params_CPP, width = 8, height = 5)


```


To better understand the mechanisms underlying the observed behavioral differences, we analyzed the parameters of the Gaussian Process Upper Confidence Bound (GP-UCB) model (@fig-GP-CUB_params). 

### Generalization $\lambda$
The parameter $\lambda$ represents the length-scale in the RBF kernel, which governs the amount of generalization, i.e., to what extent participants assume a spatial correlation between options (higher $\lambda$ = stronger generalization). Overall, the amount of generalization was very similar between groups. 

- Control vs. PD+: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="lambda")$estimate, subset(df_gpucb_params, group == 'PD+' & param=="lambda")$estimate, paired = F)`
- Control vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="lambda")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="lambda")$estimate, paired = F)`
- PD+ vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'PD+' & param=="lambda")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="lambda")$estimate, paired = F)`

### Exploration bonus $\beta$
The parameter $\beta$ represents the uncertainty bonus, i.e. how much expected rewards are positively inflated by their uncertainty (higher $\beta$ = more uncertainty-directed exploration). Controls and PD+ patients on medication did not differ, and both groups had lower beta estimates than the dopamine-depleted patients in the PD− group. These differences suggest that levodopa medication modulated the amount of uncertainty-directed exploration by restoring beta to levels comparable to those observed in controls without PD. This aligns with findings from a restless bandit paradigm, where L-Dopa reduced the amount of directed exploration in healthy volunteers, while the level of random exploration remained unaffected [@chakroun2020dopaminergic].

- Control vs. PD+: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="beta")$estimate, subset(df_gpucb_params, group == 'PD+' & param=="beta")$estimate, paired = F)`
- Control vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="beta")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="beta")$estimate, paired = F)`
- PD+ vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'PD+' & param=="beta")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="beta")$estimate, paired = F)`

### Random exploration $\tau$
The parameter $\tau$ represents the amount of decision noise, i.e. stochastic variability in the softmax decision rule (lower $\tau$ = more decision noise, i.e. more uniform distribution; conversely, $\tau \rightarrow \infty \quad \Rightarrow \quad \text{argmax (greedy)}$). There were no group differences in rge temperature paramter $\tau$, indicating comparable amounts of random exploration regardless of group.

- Control vs. PD+: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="tau")$estimate, subset(df_gpucb_params, group == 'PD+' & param=="tau")$estimate, paired = F)`
- Control vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="tau")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="tau")$estimate, paired = F)`
- PD+ vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'PD+' & param=="tau")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="tau")$estimate, paired = F)`




## Model simulations
To evaluate how well different parameter settings balance exploration and exploitation, we conducted simulations with the GP-UCB model. In these simulations, we fixed the value of $\lambda$ at 1, corresponding to the true amount of correlation in the used environments, and systematically varied the amount of random exploration ($\tau$) and the size of the uncertainty bonus ($\beta$). For each parameter we defined used equally log-spaced values, and then simulated 100 learners searching for rewards. Environments were sampled (with replacement) from the set of 40 environments used in the empirical study.


```{r}
#| label: fig-simulateModels-results
#| fig-cap: "Model simulation."
#| fig-width: 12
#| fig-height: 8

# simulation results
sim = read.csv('modelResults/simulatedModels_local_lambda_1.csv')
#sim = read.csv('modelResults/simulatedModels_local_lambda_0_5.csv')

# normalize reward
sim$meanReward = sim$mu / 50

sim_means <-  sim %>% 
  group_by(tau,beta) %>% 
  summarise(meanReward = mean(mu)) %>% 
  mutate(meanReward = meanReward/50) %>% 
  mutate(beta_log10 = log10(beta),
         tau_log10 = log10(tau)
  )


# get mean parameter estimates by group
marker <- df_gpucb_params %>%
  group_by(group, param) %>%
  summarise(#mean = mean(estimate, na.rm = TRUE), 
            median = median(estimate, na.rm = TRUE),
                             .groups = "drop") %>%
  filter(param %in% c("beta", "tau")) %>%
  pivot_wider(names_from = param, values_from = median) %>%
  mutate(beta_log10 = log10(beta), tau_log10 = log10(tau))

# get median parameter estimates by group
marker2 <-
  df_gpucb_params %>%
  select(id, group, param, estimate) %>%
  filter(param %in% c("beta", "tau")) %>%
  pivot_wider(names_from = param, values_from = estimate) %>%
  mutate(beta_log10 = log10(beta), tau_log10 = log10(tau))

# tick positions in the original scale
bx <- c(0.001, 0.01, 0.1, 1, 10, 50)
by <- c(0.001, 0.01, 0.1, 1, 10, 20)

# Control group
p_model_simulation_params_control <- 
ggplot(sim_means, aes(x = beta_log10, y = tau_log10, fill = meanReward)) +
  geom_raster() +
  scale_x_continuous(breaks = log10(bx), labels = bx, expand = c(0,0)) +
  scale_y_continuous(breaks = log10(by), labels = by, expand = c(0,0)) +
  labs(x = expression(paste('Exploration bonus ', beta)),
       y = expression(paste('Random exploration ', tau))) +
  # scale_fill_gradientn(colours = colorspace::sequential_hcl(500, "plasma"),name = "Normalized\nreward") +
  # scale_fill_viridis_c(option = "plasma", name = "Normalized\nreward") +
  scale_fill_gradientn(colours = cet_pal(5, name = "l7"), name = "Normalized\nreward") + #l3 l6 l16 i5 "rainbow"
  #coord_cartesian(xlim = c(-3,0.5), ylim=c(-3, 0.5)) +
  geom_jitter( # add individual points
    data = subset(marker2, group == "Control"),
    aes(x = beta_log10, y = tau_log10, shape = group, colour = group),
    inherit.aes = FALSE,  
    size = 3,
    fill = "#7570b3",
    shape = 21,
    stroke = 0.3,
    color = "white",
    width = 0.1
  ) +
 geom_point( # add median
    data = subset(marker, group == "Control"),
    aes(x = beta_log10, y = tau_log10, colour = group),
    inherit.aes = FALSE,
    shape = 21,        
    size = 5,          
    stroke = 1.5,
    fill = "#7570b3",
    color = "white"
  ) +
  ggtitle("Control") +
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=18),
        legend.position = "none",
        #        legend.justification = c(0, 1),
         
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 18),
        plot.title = element_text(size = 18, hjust = 0.5),
        axis.title.y = element_blank()
  )

#p_model_simulation_params_control

# PD- group
p_model_simulation_params_pd_off <- 
ggplot(sim_means, aes(x = beta_log10, y = tau_log10, fill = meanReward)) +
  geom_raster() +
  scale_x_continuous(breaks = log10(bx), labels = bx, expand = c(0,0)) +
  scale_y_continuous(breaks = log10(by), labels = by, expand = c(0,0)) +
  labs(x = expression(paste('Exploration bonus ', beta)),
       y = expression(paste('Random exploration ', tau))) +
  # scale_fill_gradientn(colours = colorspace::sequential_hcl(500, "plasma"),name = "Normalized\nreward") +
  # scale_fill_viridis_c(option = "plasma", name = "Normalized\nreward") +
   scale_fill_gradientn(colours = cet_pal(5, name = "l7"), name = "Normalized\nreward") + #l3 l16 i5 "rainbow"
  coord_cartesian(xlim = c(-3,0.5), ylim=c(-3, 0.5)) +
  geom_jitter( # add individual points
    data = subset(marker2, group == "PD-"),
    aes(x = beta_log10, y = tau_log10, shape = group, colour = group),
    inherit.aes = FALSE,  
    size = 3,
    color = "white",
    fill = "#d95f02",
    shape = 22,
    stroke = 0.3,
    width = 0.1
  ) +
 geom_point( # add median
    data = subset(marker, group == "PD-"),
    aes(x = beta_log10, y = tau_log10, colour = group),
    inherit.aes = FALSE,
    shape = 22,        
    size = 5,          
    stroke = 1.5,
    fill = "#d95f02",
    color = "white"
  ) +
  ggtitle("PD-") +
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=18),
        legend.position = "none",
        #        legend.justification = c(0, 1),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 18),
        plot.title = element_text(size = 18, hjust = 0.5)
  )

# PD+ group
p_model_simulation_params_pd_on <- 
  ggplot(sim_means, aes(x = beta_log10, y = tau_log10, fill = meanReward)) +
  geom_raster() +
  scale_x_continuous(breaks = log10(bx), labels = bx, expand = c(0,0)) +
  scale_y_continuous(breaks = log10(by), labels = by, expand = c(0,0)) +
  labs(x = expression(paste('Exploration bonus ', beta)),
       y = expression(paste('Random exploration ', tau))) +
  # scale_fill_gradientn(colours = colorspace::sequential_hcl(500, "plasma"),name = "Normalized\nreward") +
  # scale_fill_viridis_c(option = "plasma", name = "Normalized\nreward") +
   scale_fill_gradientn(colours = cet_pal(5, name = "l7"), name = "Normalized\nreward") + #l3 l16 i5 "rainbow"
  coord_cartesian(xlim = c(-3,0.5), ylim=c(-3, 0.5)) +
  geom_jitter( # add individual points
    data = subset(marker2, group == "PD+"),
    aes(x = beta_log10, y = tau_log10, shape = group, colour = group),
    inherit.aes = FALSE,  
    size = 3,
    color = "white",
    fill = "#1b9e77",
    shape = 24,
    stroke = 0.3,
    width = 0.1
  ) +
 geom_point( # add median
    data = subset(marker, group == "PD+"),
    aes(x = beta_log10, y = tau_log10, colour = group),
    inherit.aes = FALSE,
    shape = 24,        
    size = 5,          
    stroke = 1.5,
    fill = "#1b9e77",
    color = "white"
  ) +
  ggtitle("PD+") +
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=18),
        legend.position = "none",
        #        legend.justification = c(0, 1),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 18),
        plot.title = element_text(size = 18, hjust = 0.5),
        axis.title.y = element_blank()
        # axis.text.y = element_blank()
  )
# p_model_simulation_params_pd_on

# Extract legend
shared_legend <- cowplot::get_legend(
  p_model_simulation_params_control +
    theme(legend.position = "right",
          legend.title = element_text(size = 16),
          legend.text  = element_text(size = 14))
)

# combine plots
p_model_simulation_params_combined <- cowplot::plot_grid(
  p_model_simulation_params_pd_off,
  p_model_simulation_params_pd_on,
  p_model_simulation_params_control,
  nrow = 1, align = "hv", axis = "tblr", rel_widths = c(1,1,1),
  labels = NULL
)

# add  legend
p_model_simulation_params_combined <- cowplot::plot_grid(
  p_model_simulation_params_combined, shared_legend,
  ncol = 2, rel_widths = c(1, 0.10)   
)

# add title 
p_model_simulation_params <- ggdraw() +
  draw_label(
    "Model performance simulation",   
    x = 0.1, y = 0.98,               
    hjust = 0.5, vjust = 1,
    size = 24
  ) +
  draw_plot(p_model_simulation_params_combined, y = 0, height = 0.9)  

# p_model_simulation_params

# ggsave("plots/model_simulation_params.png", p_model_simulation_params, width = 14, height = 5, dpi = 300)


# zoomed in version
# combine plots
p_model_simulation_params_combined_zoom <- cowplot::plot_grid(
  p_model_simulation_params_pd_off + coord_cartesian(xlim = c(-2,0.1), ylim=c(-2, -0.5)), 
  p_model_simulation_params_pd_on + coord_cartesian(xlim = c(-2,0.1), ylim=c(-2, -0.5)), 
  p_model_simulation_params_control + coord_cartesian(xlim = c(-2,0.1), ylim=c(-2, -0.5)) ,
  nrow = 1, align = "hv", axis = "tblr", rel_widths = c(1,1,1),
  labels = NULL
)

# add  legend
p_model_simulation_params_combined_zoom <- cowplot::plot_grid(
  p_model_simulation_params_combined_zoom, shared_legend,
  ncol = 2, rel_widths = c(1, 0.10)   
)

# add title 
p_model_simulation_params_zoom <- ggdraw() +
  draw_label(
    "Model performance simulation",   
    x = 0.05, y = 0.98,               
    hjust = 0, vjust = 1,
    size = 24
  ) +
  draw_plot(p_model_simulation_params_combined_zoom, y = 0, height = 0.9)  

p_model_simulation_params_zoom
ggsave("plots/model_simulation_params_zoom.png", p_model_simulation_params_zoom, width = 14, height = 5, dpi = 300)
# ggsave("plots/model_simulation_params_zoom_lambda_0_5.png", p_model_simulation_params_zoom, width = 14, height = 5, dpi = 300)
```


# Supplementary Information {#sec-SI}

## Statistical analyses

Statistical analyses were performed using R. We report both frequentist and Bayesian statistics, using Bayes factors (BF) to quantify the relative evidence of the data in favor of the alternative hypothesis ($H_1$) over the null ($H_0$). All data and code required for reproducing the statistical analyses and figures are available at  ADD GITHUB or OSF LINK.

For parametric group comparisons, we report (paired or independent) Student's _t_-tests (two-tailed). For non-parametric comparisons we used the Mann-Whitney _U_ test or Wilcoxon signed-rank test. Bayes factors for the _t_-tests were computed with the \BayesFactor package [@BF_Morey_Rouder], using its default settings. Bayes factor for rank tests were computed following [@van2020bayesian].

Linear correlations were assessed using Pearson's $r$, with the Bayes factors computed with the _BayesFactor_ package [@BF_Morey_Rouder], using its default settings. Bayes factors for rank correlations quantified with Kendall’s tau were computed using an implementation from @van2018bayesian. 

## Supplementary computational results

### Model comparison: $R^2$
```{r}
#| fig-cap: "Predictive accuracy of GP-UCB model and lesioned variants."
#| label: fig-model_comparison-R2
#| fig-width: 14

# perform frequentist and Bayesian t-tests and make labels for plotting 
comparisons_df <- modelFits %>%
  group_by(group) %>%
  group_modify(~{
    # pariwise t-tests
    comparisons <- list(
      c("GP\nUCB", "lambda\nlesion"),
      c("GP\nUCB", "beta\nlesion"),
      c("GP\nUCB", "tau\nlesion")
    )
    
    t_res <- t_test(R2 ~ shortname, 
                    data = .x, 
                    paired = TRUE,
                    comparisons = comparisons) %>%
      add_xy_position(x = "shortname") %>%
      mutate(
        p.format = case_when(
          p < 0.001 ~ "p<.001",
          TRUE ~ paste0("p=", signif(p, 2))
        )
      )
    
    # compute Bayes Factors BF10
    t_res$BF <- purrr::pmap_dbl(
      list(t_res$group1, t_res$group2),
      function(g1, g2) {
        x1 <- .x$R2[.x$shortname == g1]
        x2 <- .x$R2[.x$shortname == g2]
        bf <- BayesFactor::ttestBF(x = x1, y = x2, paired = TRUE)
        as.numeric(BayesFactor::extractBF(bf)$bf)
      }
    )
    
    t_res
  }) %>% 
  mutate( # make BF label
    BF.format = case_when(
      BF > 100 ~ "BF>100",
      TRUE ~ paste0("BF=", signif(BF, 2))
    )) %>% 
  mutate(plot_label = paste0(p.format, ", ", BF.format)) # make plot label

# get y.positions from first comaprisons
ref_ypos <- comparisons_df %>%
  filter(group == first(levels(modelFits$group))) %>%
  pull(y.position)

# set manually
ref_ypos <- c(0.68, 0.74, 0.8)

comparisons_df <- comparisons_df %>%
  group_by(group) %>%
  mutate(y.position = ref_ypos) %>%
  ungroup()

p_model_comparison_R2 <- ggplot(modelFits, aes(x = shortname, y = R2, fill = group, shape = group, color = group)) +
  facet_wrap(~group, nrow = 1) +
  geom_boxplot(alpha = 0.2, width = 0.4, outlier.shape = NA) +  
  geom_jitter(width = 0.1, size = 1.5, alpha = 0.6) +  
  stat_summary(fun = mean, geom = "point", shape = 23, fill = "white", size = 4) +
  scale_y_continuous(name = expression("Predictive accuracy " ~ R^2),
                     breaks = c(0, 0.5, 1))+  
  scale_x_discrete("",
                   labels = c(
      "lambda\nlesion" = "λ\nlesion",
    "beta\nlesion"   = "β\nlesion",
    "tau\nlesion"    = "τ\nlesion"
    # labels = c(
    # "lambda\nlesion" = expression(atop(lambda, lesion)),
    # "beta\nlesion"   = expression(atop(beta, lesion)),
    # "tau\nlesion"    = expression(atop(tau, lesion))
    )) + 
  scale_color_manual(values = groupcolors) + 
  ggtitle("Model comparison: GP-UCB vs. lesioned models") + 
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=18),
        legend.position = "none",
        legend.justification = c(0, 1),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 18),
        plot.title = element_text(size = 24),
        plot.margin = margin(0, 0, 20, 0) # positive bottom margin, otherwise artfecat when putting later together with cowplot
  )

   
p_model_comparison_R2
ggsave("plots/S4_model_comparison_R2.png", p_model_comparison_R2, dpi=300, width = 10, height = 5)
#ggsave("plots/model_comparison.pdf", p_model_comparison, width = 10, height = 5) # ü

# plot for Computational Psychiatry Conference (CPP; Tübingen, July 2025)
# ggboxplot(modelFits, 
#           x = "shortname", 
#           y = "R2",
#           color = "group", palette = groupcolors, fill = "group", alpha = 0.2,
#           add = "jitter", jitter.size = 1, shape = "group",
#           title = "Model comparison: GP-UCB vs. lesioned models") +
#   facet_wrap(~group, nrow = 1) +
#   ylab(bquote(R^2)) +
#   xlab("") +
#   stat_pvalue_manual(
#     filter(comparisons_df, group == "Control"),
#     label = "plot_label",
#     tip.length = 0.01, bracket.size = 0.3, size = 3
#   ) +
#   stat_pvalue_manual(
#     filter(comparisons_df, group == "PD+"),
#     label = "plot_label",
#     tip.length = 0.01, bracket.size = 0.3, size = 3
#   ) +
#   stat_pvalue_manual(
#     filter(comparisons_df, group == "PD-"),
#     label = "plot_label",
#     tip.length = 0.01, bracket.size = 0.3, size = 3
#   ) +
#   theme_classic()+
#   theme(strip.background = element_blank(),  
#         strip.text = element_text(color = "black", size=12),
#         legend.position = "none",
#         plot.title = element_text(size = 24),
#         axis.text= element_text(colour="black", size = 14),
#         axis.title= element_text(colour="black", size = 14)
#   ) 
# 
# ggsave("plots/model_comparison_CPP.png", p_model_comparison_CPP, dpi=300, width = 9, height = 5)


```

### Model comparison $R^2$: Control
- GP-UCB vs. lambda lesion: `r ttestPretty(subset(modelFits, group == 'Control' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'Control' & shortname == "lambda\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`
- GP-UCB vs. beta lesion: `r ttestPretty(subset(modelFits, group == 'Control' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'Control' & shortname == "beta\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`
- GP-UCB vs. tau lesion: `r ttestPretty(subset(modelFits, group == 'Control' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'Control' & shortname == "tau\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`

### Model comparison $R^2$: PD+
- GP-UCB vs. lambda lesion: `r ttestPretty(subset(modelFits, group == 'PD+' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'PD+' & shortname == "lambda\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`
- GP-UCB vs. beta lesion: `r ttestPretty(subset(modelFits, group == 'PD+' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'PD+' & shortname == "beta\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`
- GP-UCB vs. tau lesion: `r ttestPretty(subset(modelFits, group == 'PD+' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'PD+' & shortname == "tau\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`

### Model comparison $R^2$: PD-
- GP-UCB vs. lambda lesion: `r ttestPretty(subset(modelFits, group == 'PD-' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'PD-' & shortname == "lambda\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`
- GP-UCB vs. beta lesion: `r ttestPretty(subset(modelFits, group == 'PD-' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'PD-' & shortname == "beta\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`
- GP-UCB vs. tau lesion: `r ttestPretty(subset(modelFits, group == 'PD-' & shortname == "GP\nUCB" )$R2, subset(modelFits, group == 'PD-' & shortname == "tau\nlesion" )$R2,var.equal = TRUE, paired = TRUE)`

# Relations of model parameters to performance

We assessed the correlation (Kendall's tau, because it's invariant against log transformation) of GP-UCB parameter estimates with performance (mean reward).

```{r}

# mean reward per subject across all trials and rounds (practice and bonus round excluded)
df_mean_reward_subject <- dat %>% 
  filter(trial != 0 & round %in% 2:9) %>% # exclude first (randomly revealed) tile and practice round and bonus round
  group_by(id) %>% 
  summarise(group = first(group),
            sum_reward = sum(z),
            mean_reward = mean(z), 
            sd_reward = sd(z)) 

df_params_performance <- df_gpucb_params %>% 
  left_join(df_mean_reward_subject, by = c("id", "group"))

df_params_performance_wide <- df_gpucb_params %>% 
  pivot_wider(names_from = param, values_from = estimate ) %>% 
  left_join(df_mean_reward_subject, by = c("id", "group"))

```


The amount of generalization was positively related with obtained rewards, showing that participants who successfully learned about the spatially correlation of rewards performed better. The uncertainty bonus $\beta$ was negatively correlated with performance, demonstrating that an overreliance on exploration impairs efficient reward accumulation. The amount of random temperature $\tau$ was not related to obtained rewards.

```{r}
#| fig-cap: "Correlation of GP-UCB parameters with obtained mean reward across all trials and rounds. Each dot is one participant. The insets show the correlations for a restricted parameter range from 0 to 1."
#| label: fig-params_reward_cor
#| fig-width: 13
#| fig-height: 5


# plot correlation lambda and reward 
p_lambda_reward <- 
  ggplot(subset(df_params_performance, param == "lambda"), aes(x = estimate, y = mean_reward)) +
  geom_point(aes(color = group, shape = group, fill = group)) +
  # geom_smooth(method = "lm", color = "black", linetype = "dashed", fill = "lightgray", se = TRUE) + # one regression line
  geom_smooth(aes(color = group, fill = group), method = "lm", linetype = "dashed", alpha = 0.2, se = TRUE) +  # one regrssion line per group
  stat_cor(method = "kendall", cor.coef.name = expression(r[tau]), label.x.npc = "left",  label.y = 0.4,  size=5, p.accuracy=0.001) +
  labs(
    title = expression("Generalization " * lambda),
    x = "Estimate (log)",
    y = "Mean normalized reward"
  ) +
  # scale_x_log10(name = "Estimate (log scale)", breaks = c(0.1, 1, 10), labels = c("0.1", "1", "10"), limits = c(0.1, 2)) +
  scale_x_continuous(name = "Estimate", breaks = c(0, 1, 2), labels = c("0", "1", "2"), limits = c(0, 1.45)) +
  scale_y_continuous(breaks = seq(0, 1, .1), limits = c(0.4, 0.85)) +
  scale_fill_manual(values = groupcolors) + 
  scale_color_manual(values = groupcolors) +
  theme_classic() +
  theme(
    plot.title = element_text(hjust = 0.5, size=18),
    legend.title = element_blank(),
    axis.title = element_text(color = "black", size = 14),
    axis.text = element_text(color = "black", size = 14),
    legend.position = c(0.12, 0.9), #bottom: 0.15 top:0.9
    legend.text = element_text(size = 14),
    legend.key.size = unit(0.8, "lines"),
    legend.margin = margin(0, 0, 0, 0),       
  legend.box.margin = margin(0, 0, 0, 0),
  plot.margin = margin(10, 0, 0, 0), 
  legend.key = element_rect(fill = NA, colour = NA)

  )

# ggsave("plots/cor_lambda_reward.png", p_lambda_reward, dpi = 300, width = 8, height = 5)

# plot correlation between exploration bonus beta (log10) and reward  
p_beta_reward <-
  ggplot(subset(df_params_performance, param == "beta"), aes(x = estimate, y = mean_reward)) +
   geom_point(aes(color = group, shape = group, fill = group)) +
  # geom_smooth(method = "lm", color = "black", linetype = "dashed", fill = "lightgray", se = TRUE) + # one regression line
  geom_smooth(aes(color = group, fill = group), method = "lm", linetype = "dashed", alpha = 0.2, se = TRUE) +  # one regrssion line per group
  stat_cor(method = "kendall", cor.coef.name = expression(r[tau]), label.x.npc = "left",  label.y = 0.4,  p.accuracy=0.001, size=5) + #label.x = 0.00,  label.y = 0.4,
  labs(
    title = expression("Exploration bonus " * beta),
    x = "Estimate (log)",
    y = " "
  ) +
  scale_x_log10(name = "Estimate (log scale)",  breaks = c(0.01, 0.1, 1, 10), labels = c("0.01","0.1", "1", "10"), limits = c(0.005, 70)) +
  scale_y_continuous(breaks = seq(0, 1, .1), limits = c(0.4, 0.85)) +
  scale_fill_manual(values = groupcolors) + 
  scale_color_manual(values = groupcolors) +
  theme_classic() +
  theme(
    plot.title = element_text(hjust = 0.5, size=18),
    legend.title = element_blank(),
    axis.title = element_text(color = "black", size = 14),
    axis.text = element_text(color = "black", size = 14),
    legend.position = "none",
    plot.margin = margin(10, 0, 0, 0)
  )

# ggsave("plots/cor_beta_reward.png", p_beta_reward, dpi = 300, width = 8, height = 5)


# plot correlation between amount of random exploration (temperature tau of softmax choice rule) and reward  
p_tau_reward <-
  ggplot(subset(df_params_performance, param == "tau"), aes(x = estimate, y = mean_reward)) +
  geom_point(aes(color = group, shape = group, fill = group)) +
  # geom_smooth(method = "lm", color = "black", linetype = "dashed", fill = "lightgray", se = TRUE) + # one regression line
  geom_smooth(aes(color = group, fill = group), method = "lm", linetype = "dashed", alpha = 0.2, se = TRUE) +  # one regrssion line per group
  stat_cor(method = "kendall", cor.coef.name = expression(r[tau]), label.x.npc = "left",  label.y = 0.4, p.accuracy=0.01, size=5) +
  labs(
    title = expression("Random exploration " * tau),
    x = "Estimate (log)",
    y = " "
  ) +
  scale_x_log10(name = "Estimate (log scale)",  breaks = c(0.01, 0.1, 1, 10), labels = c("0.01","0.1", "1", "10"), limits = c(0.005, 20)) +
  scale_y_continuous(breaks = seq(0, 1, .1), limits = c(0.4, 0.85)) +
  scale_fill_manual(values = groupcolors) + 
  scale_color_manual(values = groupcolors) +
  theme_classic() +
  theme(
    plot.title = element_text(hjust = 0.5, size=18),
    legend.title = element_blank(),
    axis.title = element_text(color = "black", size = 14),
    axis.text = element_text(color = "black", size = 14),
    legend.position = "none",
    plot.margin = margin(10, 0, 0, 0)
  )

# ggsave("plots/cor_tau_reward.png", p_tau_reward, dpi = 300, width = 8, height = 5)

# put plots together and add title
p_parameters_reward <- grid.arrange(
  grobs = list(p_lambda_reward, p_beta_reward, p_tau_reward),
  nrow = 1,
  top = textGrob(
    "         GP-UCB parameters: Relations to performance", 
    x = 0,            
    hjust = 0,        
    gp = gpar(fontsize = 24)
  ),
  padding = unit(0.5, "lines") 
)

p_parameters_reward

ggsave("plots/parameters_reward.png", p_parameters_reward, dpi = 300, width = 13, height = 4)
 
# # plot correlation between parameter estimates and mean reward, with inset for smaller estimate range 
# main_plot <- ggscatter(df_params_performance, x = "estimate", y = "mean_reward",
#                        add = "reg.line",  
#                        add.params = list(color = "darkred", fill = "lightgray"), 
#                        conf.int = TRUE 
# ) +
#   facet_wrap(~param, scales = "free_x") +
#   stat_cor(method = "pearson", label.x = c(0,3,3), label.y = 45) +
#   ggtitle("Correlation between GP-UCB parameters and reward") +
#   scale_y_continuous("Mean reward", breaks = seq(0,45,10)) +
#   xlab("Estimate") +
#   theme_classic() +
#   theme(strip.background = element_blank(),  
#         strip.text = element_text(color = "black", size=12),
#         legend.title = element_blank(),
#         axis.title = element_text(color = "black", size=14),
#         axis.text = element_text(color = "black", size=14))
# 
# # Inset-Plot für beta (nur Werte 0-1)
# inset_beta <- 
#   ggscatter(df_params_performance %>% filter(param == "beta" & estimate > 0 & estimate <= 1), 
#             x = "estimate", y = "mean_reward",
#             add = "reg.line",  
#             add.params = list(color = "darkred", fill = "lightgray"), 
#             conf.int = TRUE 
#   ) +
#   stat_cor(method = "pearson", label.x = 0.1, label.y = 42, size = 3) +
#   scale_x_continuous( breaks = c(0,0.25, 0.5, 0.75), labels = c("0.0", "0.25", "0.5", "0.75")) +# breaks = seq(0,1,0.1),
#   theme_classic() +
#   theme(axis.title = element_blank(),
#         strip.background = element_blank(), 
#         strip.text = element_blank(), 
#         legend.position = "none")
# 
# # Inset-Plot für tau (nur Werte 0-1)
# inset_tau <- 
#   ggscatter(df_params_performance %>% filter(param == "tau" & estimate > 0 & estimate <= 1), 
#             x = "estimate", y = "mean_reward",
#             add = "reg.line",  
#             add.params = list(color = "darkred", fill = "lightgray"), 
#             conf.int = TRUE 
#   ) +
#   stat_cor(method = "pearson", label.x = 0.05, label.y = 42, size = 3) +
#   scale_x_continuous(breaks = seq(0,1,0.1)) +
#   theme_classic() +
#   theme(axis.title = element_blank(),strip.background = element_blank(), strip.text = element_blank(), legend.position = "none")
# 
# 
# # Hauptplot mit Insets für beta und tau
# p_GP_UCB_params_cor_reward_inset <- 
#   ggdraw(main_plot) +
#   draw_plot(inset_beta, x = 0.5, y = 0.45, width = 0.15, height = 0.35) +
#   draw_plot(inset_tau, x = 0.8, y = 0.45, width = 0.15, height = 0.35)
# 
# p_GP_UCB_params_cor_reward_inset
# 
# ggsave("plots/GP-UCB_params_cor_reward.png", p_GP_UCB_params_cor_reward_inset, width = 12, height= 4, dpi=300)


```



## Generalization $\lambda$

Overall, the extent of generalization was positively related to performance, suggesting that participants who stronger generalized obtained more rewards:

-   Overall: `r corTestPretty(subset(df_params_performance, param == "lambda")$estimate_log10, subset(df_params_performance, param == "lambda")$mean_reward, method = "kendall")`

Analysis of parameter estimates on the group level showed that this overall relation was primarily driven by PD+ patients, who showed a strong relation, whereas there was no relation in controls or PD- patients: 

-   Control: `r corTestPretty(subset(df_params_performance, param == "lambda" & group == "Control")$estimate, subset(df_params_performance, param == "lambda" & group == "Control")$mean_reward, method = "kendall")`
-   PD+: `r corTestPretty(subset(df_params_performance, param == "lambda" & group == "PD+")$estimate, subset(df_params_performance, param == "lambda" & group == "PD+")$mean_reward, method = "kendall")`
-   PD-: `r corTestPretty(subset(df_params_performance, param == "lambda" & group == "PD-")$estimate, subset(df_params_performance, param == "lambda" & group == "PD-")$mean_reward, method = "kendall")`


## Exploration bonus $\beta$
The exploration bonus $\beta$ driving uncertainty-directed correlation was negatively related to performance, suggesting that participants who explore too much at the cost of exploiting known high-value options achieve lower performance:

-   Overall: `r corTestPretty(subset(df_params_performance, param == "beta")$estimate_log10, subset(df_params_performance, param == "beta")$mean_reward, method = "kendall")`

Analysis of parameter estimates on the group level showed that this overall relation was primarily driven by PD+ patients, who showed a strong relation, whereas there was no relation in controls or PD- patients: 

-   Control: `r corTestPretty(subset(df_params_performance, param == "beta" & group == "Control")$estimate, subset(df_params_performance, param == "beta" & group == "Control")$mean_reward, method = "kendall")`
-   PD+: `r corTestPretty(subset(df_params_performance, param == "beta" & group == "PD+")$estimate, subset(df_params_performance, param == "beta" & group == "PD+")$mean_reward, method = "kendall")`
-   PD-: `r corTestPretty(subset(df_params_performance, param == "beta" & group == "PD-")$estimate, subset(df_params_performance, param == "beta" & group == "PD-")$mean_reward, method = "kendall")`


## Random exploration $\tau$
The temperature parameter of the softmax choice rule $\tau$, representig random exploration, was not related to performance, suggesting that participants who explore too much at the cost of exploiting known high-value options achieve lower performance:

-   Overall: `r corTestPretty(subset(df_params_performance, param == "tau")$estimate_log10, subset(df_params_performance, param == "tau")$mean_reward, method = "kendall")`

Analysis of parameter estimates on the group level showed that this overall relation was primarily driven by PD+ patients, who showed a strong relation, whereas there was no relation in controls or PD- patients: 

-   Control: `r corTestPretty(subset(df_params_performance, param == "tau" & group == "Control")$estimate, subset(df_params_performance, param == "tau" & group == "Control")$mean_reward, method = "kendall")`
-   PD+: `r corTestPretty(subset(df_params_performance, param == "tau" & group == "PD+")$estimate, subset(df_params_performance, param == "tau" & group == "PD+")$mean_reward, method = "kendall")`
-   PD-: `r corTestPretty(subset(df_params_performance, param == "tau" & group == "PD-")$estimate, subset(df_params_performance, param == "tau" & group == "PD-")$mean_reward, method = "kendall")`

## Regression: Model parameters and reward
We also performed a regression analysis where we included all model parameters together with group as predictors. We ran one model with main effects only and one full model including all interaction terms. Note, however, that these analyses should be interpreted with caution, as we would not theoretically expect linear relations between model parameters and reward; rather, non-linear (e.g., inverse U-shaped) relations are more plausible.

```{r}
#| label: tbl-performance-parameters
#| tbl-cap: "Regression results: Performance (obtained rewards) as function of group and model parameters (log scale)."

# main effects + interaction
df_params_performance_wide <- 
  df_params_performance %>% 
    filter(ModelName == "GP-UCB") %>% 
    select(id, group, mean_reward, param, estimate_log10) %>%
    distinct(id, group, param, .keep_all = TRUE) %>%   # drop duplicates if any
  pivot_wider(names_from = param, values_from = estimate_log10) %>%
  drop_na(beta, tau, lambda)   

lm_performance_parameters_log <- lm(mean_reward ~ group * (lambda + beta + tau), data =  df_params_performance_wide)

# tab_model(lm_performance_parameters_log)

# main effects only
lm_performance_parameters_log_main_effects <- lm(mean_reward ~ group + lambda + beta + tau, data =  df_params_performance_wide)

# tab_model(lm_performance_parameters_log_main_effects)

tab_model(
  lm_performance_parameters_log_main_effects, 
  lm_performance_parameters_log,
  dv.labels = c("Main effects only", "Full model")
)

```


# Article figure

The following code generates Figure 3 from the article.

## Figure 3. Computational results
```{r}
#| label: fig-computational-results
#| fig-cap: "Computational results."
#| fig-width: 14
#| fig-height: 17

# p_model_comparison <- plot_grid(
#   p_pxp,
#   p_classification_participants,
#   ncol = 2,
#   labels = NULL
#   # rel_widths = c(0.4, 0.6)
# ) +
#   draw_figure_label(label = "Model comparison", position = "top.left", size = 18)



# first row has model comparison: p_pxp and p_classification_participants 
p_model_comparison <- plot_grid(
  p_pxp,
  p_classification_participants,
  ncol = 2,
  labels = c("a", "b"),      
  label_size = 22,
  rel_widths = c(0.55,0.45)
)

# Gesamt-Layout
fig3_computational_results <- cowplot::plot_grid(
  p_model_comparison,
  p_gpucb_params,
  p_model_simulation_params_zoom,
  ncol = 1,
  # labels = "auto",
  labels = c("", "c", "d"),  
  label_y = 1.02,
  # label_x = 0.1,   
  label_size = 22,
  align = "h",
  axis = "l",
  rel_heights = c(2/3, 1, 1)
)

ggsave("plots/fig3_computational_results.png", fig3_computational_results, dpi=300, height=12, width=15)

```

# Relations of model parameters and clinical indicators

## Bivariate correlations
```{r}
# 
df_params_clinical_indicators <- df_gpucb_params %>% 
  left_join(dat_sample, by= c("id", "group")) %>% 
  select(id, group, param, estimate, hoehn_yahr, BDI, MMSE) %>%
  pivot_wider(names_from = param, values_from = estimate) %>% 
  ungroup()


# control group
cor_matrix_controls <- df_params_clinical_indicators %>% 
  filter(group == "Control") %>% 
  select(BDI, MMSE, lambda, beta, tau) %>% 
  cor(method = "kendall", use = "pairwise.complete.obs")

p_cor_matrix_controls <- 
  ggcorrplot(cor_matrix_controls,
           # method = "circle",     
           type   = "lower",      
           lab    = TRUE,         
           show.legend = FALSE,    
           title  = "Controls") +
    scale_fill_gradient2(
    low = "darkred",   
    mid = "white",     
    high = "darkgreen",
    midpoint = 0,
    limit = c(-1, 1)
  ) +
  theme_classic() +
  theme(legend.position = "none",
        axis.text = element_text(size = 14),
        plot.title = element_text(size = 16, hjust = 0.5),
        axis.title = element_blank()
  ) +
  scale_x_discrete("",
    labels = c(
      "lambda" = "λ",
      "beta"   = "β",
      "tau"    = "τ",
      "hoehn_yahr" = "Hoehn–Yahr",
      "BDI"    = "BDI",
      "MMSE"   = "MMSE"
    )) +
  scale_y_discrete("",
    labels = c(
      "lambda" = "λ",
      "beta"   = "β",
      "tau"    = "τ",
      "hoehn_yahr" = "Hoehn–Yahr",
      "BDI"    = "BDI",
      "MMSE"   = "MMSE"
    ))

# PD+ patients on medication 
cor_matrix_pd_plus <- df_params_clinical_indicators %>% 
  filter(group == "PD+") %>% 
  select(hoehn_yahr, BDI, MMSE, lambda, beta, tau) %>% 
  cor(method = "pearson", use = "pairwise.complete.obs")

p_cor_matrix_pd_plus <- 
  ggcorrplot(cor_matrix_pd_plus,
           # method = "circle",     
           type   = "lower",      
           lab    = TRUE,         
           show.legend = FALSE,    
           title  = "PD+") +
    scale_fill_gradient2(
    low = "darkred",   
    mid = "white",     
    high = "darkgreen",
    midpoint = 0,
    limit = c(-1, 1)
  ) +
  theme_classic() +
  theme(legend.position = "none",
        axis.text = element_text(size = 14),
        plot.title = element_text(size = 16, hjust = 0.5),
        axis.title = element_blank()
  ) +
  scale_x_discrete("",
    labels = c(
      "lambda" = "λ",
      "beta"   = "β",
      "tau"    = "τ",
      "hoehn_yahr" = "Hoehn–Yahr",
      "BDI"    = "BDI",
      "MMSE"   = "MMSE"
    )) +
  scale_y_discrete("",
    labels = c(
      "lambda" = "λ",
      "beta"   = "β",
      "tau"    = "τ",
      "hoehn_yahr" = "Hoehn–Yahr",
      "BDI"    = "BDI",
      "MMSE"   = "MMSE"
    ))



# PD- patients off medication 
cor_matrix_pd_minus <- df_params_clinical_indicators %>% 
  filter(group == "PD-") %>% 
  select(hoehn_yahr, BDI, MMSE, lambda, beta, tau) %>% 
  cor(method = "pearson", use = "pairwise.complete.obs")

p_cor_matrix_pd_minus <- 
  ggcorrplot(cor_matrix_pd_minus,
           # method = "circle",     
           type   = "lower",      
           lab    = TRUE,         
           show.legend = FALSE,    
           title  = "PD-") +
    scale_fill_gradient2(
    low = "darkred",   
    mid = "white",     
    high = "darkgreen",
    midpoint = 0,
    limit = c(-1, 1)
  ) +
  theme_classic() +
  theme(legend.position = "none",
        axis.text = element_text(size = 14),
        plot.title = element_text(size = 16, hjust = 0.5),
        axis.title = element_blank()
  ) +
  scale_x_discrete("",
    labels = c(
      "lambda" = "λ",
      "beta"   = "β",
      "tau"    = "τ",
      "hoehn_yahr" = "Hoehn–Yahr",
      "BDI"    = "BDI",
      "MMSE"   = "MMSE"
    )) +
  scale_y_discrete("",
    labels = c(
      "lambda" = "λ",
      "beta"   = "β",
      "tau"    = "τ",
      "hoehn_yahr" = "Hoehn–Yahr",
      "BDI"    = "BDI",
      "MMSE"   = "MMSE"
    ))

p_cor_matrix <- cowplot::plot_grid(
  p_cor_matrix_pd_minus,
  p_cor_matrix_pd_plus,
  p_cor_matrix_controls,
  nrow = 1, align = "hv", axis = "tblr", rel_widths = c(1,1,1),
  labels = NULL
)

p_cor_matrix2 <- p_cor_matrix + draw_figure_label(label = "Correlation of model parameters and clinical indicators (Kendall's τ)", position = "top.left", size = 18)

ggsave("plots/cor_matrix_params_indicators.png", p_cor_matrix2, width = 12, height = 4)
```

## Regression analyses

GP-UCB model parameters $\lambda$ (amount of generalization), $\beta$ (exploration bonus), and $\tau$ (amount of random exploration) acorss allparticipants
```{r}
#| tbl-cap: "Linear regression with model GP-UCB model paramteres as function of group and depression level (BDI-II) and cognitive functioning (MMSE). All participants "

# for now, random intercepts only, Random intercept + random slope not stable

# fit models: main effects only
lm_tau_bdi_mmse <- lm(log(tau) ~ group + BDI + MMSE , 
                             data = df_params_clinical_indicators)

lm_beta_bdi_mmse <- lm(log(beta) ~ group + BDI + MMSE , 
                             data = df_params_clinical_indicators)

lm_lambda_bdi_mmse <- lm(lambda ~ group + BDI + MMSE , 
                             data = df_params_clinical_indicators)


tab_model(lm_lambda_bdi_mmse, lm_beta_bdi_mmse, lm_tau_bdi_mmse)


# fit models: main effects + interactions with group
lm_tau_bdi_mmse <- lm(log(tau) ~ group *(BDI + MMSE), 
                             data = df_params_clinical_indicators)

lm_beta_bdi_mmse <- lm(log(beta) ~ group * (BDI + MMSE), 
                             data = df_params_clinical_indicators)

lm_lambda_bdi_mmse <- lm(lambda ~ group * (BDI + MMSE), 
                             data = df_params_clinical_indicators)


tab_model(lm_lambda_bdi_mmse, lm_beta_bdi_mmse, lm_tau_bdi_mmse)

```

GP-UCB model parameters $\lambda$ (amount of generalization), $\beta$ (exploration bonus), and $\tau$ (amount of random exploration); PD patients only.
```{r}
#| tbl-cap: "Linear regression with model GP-UCB model paramteres as function of group and depression level (BDI-II) and cognitive functioning (MMSE). PD patients only."

# for now, random intercepts only, Random intercept + random slope not stable

# fit models: main effects only
lm_tau_bdi_mmse_hy <- lm(log(tau) ~ group + BDI + MMSE + hoehn_yahr,
                             data = subset(df_params_clinical_indicators, group != "Control"))

lm_beta_bdi_mmse_hy <- lm(log(beta) ~ group + BDI + MMSE + hoehn_yahr,
                             data = subset(df_params_clinical_indicators, group != "Control"))

lm_lambda_bdi_mmse_hy <- lm(lambda ~ group + BDI + MMSE + hoehn_yahr, 
                             data = subset(df_params_clinical_indicators, group != "Control"))

tab_model(lm_lambda_bdi_mmse_hy, lm_beta_bdi_mmse_hy, lm_tau_bdi_mmse_hy)


# fit models: main effects and interactions
lm_tau_bdi_mmse_hy <- lm(log(tau) ~ group * (BDI + MMSE + hoehn_yahr),
                             data = subset(df_params_clinical_indicators, group != "Control"))

lm_beta_bdi_mmse_hy <- lm(log(beta) ~ group * (BDI + MMSE + hoehn_yahr),
                             data = subset(df_params_clinical_indicators, group != "Control"))

lm_lambda_bdi_mmse_hy <- lm(lambda ~ group * (BDI + MMSE + hoehn_yahr), 
                             data = subset(df_params_clinical_indicators, group != "Control"))

tab_model(lm_lambda_bdi_mmse_hy, lm_beta_bdi_mmse_hy, lm_tau_bdi_mmse_hy)

```



# Model params of participants best explained by GP-UCB model

For this analysis we only consider participants who were best explained by the GP-UCB model. The results are consistent with the same analyses performed with the full sample above: no substantial differences in amount of generalization $\lambda$, marked differences in terms of the exploration bonus $\beta$, and no differences in terms of random exploration $\tau$. The only difference is that we found a difference between the control and off-medication group in the extent of generalization when using the full sample, whereas we found no difference when only considering the subset of participants best accounted for by the GP-UCB model.

### Generalization $\lambda$
The parameter $\lambda$ represents the length-scale in the RBF kernel, which governs the amount of generalization, i.e., to what extent participants assume a spatial correlation between options (higher $\lambda$ = stronger generalization). Overall, the amount of generalization was very similar between groups. 

- Control vs. PD+: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="lambda" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD+' & param=="lambda" & best_ModelName == "GP-UCB")$estimate, paired = F)`
- Control vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="lambda" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="lambda" & best_ModelName == "GP-UCB")$estimate, paired = F)`
- PD+ vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'PD+' & param=="lambda" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="lambda" & best_ModelName == "GP-UCB")$estimate, paired = F)`

### Exploration bonus $\beta$
The parameter $\beta$ represents the uncertainty bonus, i.e. how much expected rewards are positively inflated by their uncertainty (higher $\beta$ = more uncertainty-directed exploration). Controls and PD+ patients on medication did not differ, and both groups had lower beta estimates than the dopamine-depleted patients in the PD− group. These differences suggest that levodopa medication modulated the amount of uncertainty-directed exploration by restoring beta to levels comparable to those observed in controls without PD. This aligns with findings from a restless bandit paradigm, where L-Dopa reduced the amount of directed exploration in healthy volunteers, while the level of random exploration remained unaffected [@chakroun2020dopaminergic].

- Control vs. PD+: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="beta" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD+' & param=="beta" & best_ModelName == "GP-UCB")$estimate, paired = F)`
- Control vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="beta" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="beta" & best_ModelName == "GP-UCB")$estimate, paired = F)`
- PD+ vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'PD+' & param=="beta" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="beta" & best_ModelName == "GP-UCB")$estimate, paired = F)`

### Random exploration $\tau$
The parameter $\tau$ represents the amount of decision noise, i.e. stochastic variability in the softmax decision rule (lower $\tau$ = more decision noise, i.e. more uniform distribution; conversely, $\tau \rightarrow \infty \quad \Rightarrow \quad \text{argmax (greedy)}$). There were no group differences in rge temperature paramter $\tau$, indicating comparable amounts of random exploration regardless of group.

- Control vs. PD+: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="tau" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD+' & param=="tau" & best_ModelName == "GP-UCB")$estimate, paired = F)`
- Control vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'Control' & param=="tau" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="tau" & best_ModelName == "GP-UCB")$estimate, paired = F)`
- PD+ vs. PD-: `r ranktestPretty(subset(df_gpucb_params, group == 'PD+' & param=="tau" & best_ModelName == "GP-UCB")$estimate, subset(df_gpucb_params, group == 'PD-' & param=="tau" & best_ModelName == "GP-UCB")$estimate, paired = F)`


```{r}
#| fig-cap: "Parameter estimates of GP-UCB model, estimated through leave-one-round-out cross validation. Each dot is one participant. Only participants are included who were best described by the GP-UCB model."
#| label: fig-GP-CUB_params_subset
#| fig-width: 12


df_gpucb_params_subset <- modelFits %>% 
  filter(best_ModelName == "GP-UCB") %>% 
  filter(kernel=='GP' & acq == 'UCB') %>% 
  pivot_longer(c('lambda', 'beta', 'tau'), names_to = 'param', values_to = 'estimate') %>% 
  mutate(param = factor(param, levels = c('lambda', 'beta', 'tau'))) 


ggboxplot(df_gpucb_params_subset, 
          x = "group", 
          y = "estimate",
          color = "group", palette =groupcolors, fill = "group", alpha = 0.2,
          add = "jitter", jitter.size = 0.5, shape = "group", title = "GP-UCB parameter estimates") +
  facet_wrap(~param, nrow = 1) +
  scale_y_log10(breaks = c(0.01, 0.1, 1, 10, 100), labels = c("0.01", "0.1", "1", "10", "100")) +
  ylab("Estimate (log scale") +
  xlab("") +
  stat_summary(fun = mean, geom="point", shape = 23, fill = "white", size=2) +
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=12),
        legend.position = "none",
        plot.title = element_text(size = 18)
  )
  
# ggsave("plots/GP-UCB_params_subset.png", width = 9, height = 5)

```


# Session Information

<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne">

Session Information

</button>

:::: {#collapseOne .accordion-collapse .collapse}
<div>

```{r}
sessionInfo()
```

</div>
::::