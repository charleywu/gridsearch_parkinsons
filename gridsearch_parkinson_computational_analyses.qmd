---
title: " Exploration and Exploitation in Parkinson’s Disease: Computational Analyses"
author: 
  - name: Björn Meder
    affiliation: Health and Medical University, Potsdam, Germany
  - name: Martha Sterf
    affiliation: Medical School Berlin, Berlin, Germany
  - name: Charley M. Wu
    affiliation: University of Tübingen, Tübingen, Germany
  - name: Matthias Guggenmos
    affiliation: Health and Medical University, Potsdam, Germany
  
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    toc: true
    theme: zephyr
    code-fold: true
    toc-location: left-body
    classoption: fleqn
    lightbox: true
    number-sections: true
    number-figures: true
    self-contained: true
    grid:
      sidebar-width: 300px
      body-width: 1000px
      margin-width: 200px
      gutter-width: 1.5rem
  pdf:
    toc: true
    number-sections: true
    number-figures: true
  docx:
    toc: true
    number-sections: true
    number-figures: true
editor: 
  default: source
editor_options: 
  chunk_output_type: console
bibliography: gridsearch_parkinson.bib
csl: https://www.zotero.org/styles/apa
cite-method: citeproc
---


```{r, results = "hide", message=FALSE}
# Housekeeping: Load packages and helper functions
# Housekeeping
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.align='center')

options(knitr.kable.NA = '')

packages <- c('gridExtra', 'BayesFactor', 'tidyverse', "RColorBrewer", "lme4", "sjPlot", "lsr", "brms", "kableExtra", "afex", "emmeans", "viridis", "ggpubr", "hms", "scales", "cowplot")
lapply(packages, require, character.only = TRUE)

set.seed(0815)

# file with various statistical functions, among other things it provides tests for Bayes Factors (BFs)
source('statisticalTests.R')

# Wrapper for brm models such that it saves the full model the first time it is run, otherwise it loads it from disk
run_model <- function(expr, modelName, path='brm', reuse = TRUE) {
  path <- paste0(path,'/', modelName, ".brm")
  if (reuse) {
    fit <- suppressWarnings(try(readRDS(path), silent = TRUE))
  }
  if (is(fit, "try-error")) {
    fit <- eval(expr)
    saveRDS(fit, file = path)
  }
  fit
}


# Setting some plotting params
w_box          <- 0.2      # width of boxplot, also used for jittering points and lines    
line_jitter    <- w_box / 2
xAnnotate      <- -0.3

# jitter params
jit_height  <- 0.01
jit_width   <- 0.05
jit_alpha   <- 0.6

# colors for age groups
groupcolors <- c("#1b9e77", "#d95f02", "#7570b3")

```

```{r}
# read data
dat       <- read_delim("data/data_gridsearch_parkinson.csv", 
                        delim = ",",
                        col_types = cols(
                          id = readr::col_factor(),
                          group = readr::col_factor(),
                          gender = readr::col_factor(),
                          z = col_double(),        
                          zscaled = col_double(),
                          hoehn_yahr = col_double()
                        ))

# length(unique(dat$id))
# clean up
dat <- dat %>%
  select(-condition, -comments) %>% 
  mutate(group = case_match(group,
                            "PPD-" ~ "PD-",
                            "PPD+" ~ "PD+",
                            "PNP" ~ "PNP",
                            .default = NA)) %>% 
  mutate(group = factor(group, levels = c("PNP", "PD+", "PD-")))  %>% 
  mutate(type_choice = factor(type_choice, levels = c("Repeat", "Near", "Far")))  %>% 
  mutate(gender = recode(gender, "w" = "f")) %>% 
  rename(MMSE = `mini_mental`) %>% 
  mutate(last_ldopa = if_else(group != "PNP", as_hms(last_ldopa), as_hms(NA)),
         next_ldopa = if_else(group != "PNP", as_hms(next_ldopa), as_hms(NA)),
         time_exp = if_else(group != "PNP", as_hms(time_exp), as_hms(NA))) %>% 
  mutate(time_since_ldopa = as.numeric(time_exp - last_ldopa, unit = "mins"))

# get subject information
df_sample <- dat %>% 
  select(id, age, gender,group,BDI,MMSE,hoehn_yahr,last_ldopa,next_ldopa,time_exp,time_since_ldopa) %>% 
  group_by(id) %>%
  slice_head(n = 1) %>% 
  arrange(group)

# head(dat) %>%
#   kable("html", caption = "Behavioral data.") %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>% 
#   scroll_box(width = "100%", height = "300px")

```
# Computational Analyses

Complementing the behavioral analyses, we study exploration and exploitation in PD through the lens of a computational model, the Gaussian Process Upper Confidence Bound (GP-UCB) model. This model integrates similarity-based generalization with two distinct exploration mechanisms: _directed exploration_, which seeks to reduce uncertainty about rewards, and _random exploration_, which adds stochastic noise to the search process without being directed towards a particular goal. In previous research using the same paradigm, this model has provided the best account of human behavior and enabled the decomposition of exploration into distinct mechanisms
[@Wu:2018grid; @Schulz:2019kwg; @Wu:2020neurogrid; @Meder2021_ExplorationChildren; @giron2023developmental; @Wu_et_al-generalization].


## Gaussian Process Upper Confidence Bound (GP-UCB) Model

The GP-UCB model comprises three components:

1. a _learning model_, which uses Bayesian inference to generate predictions about the rewards associated with each option (tile),
2. a _sampling strategy_, which uses reward expectations and associated uncertainty to evaluate how promising each option is, and
3. a _choice rule_, which converts options' values into choice probabilities.

::: callout-note
Add details
:::
### Learning Model
### Sampling Strategy
### Choice rule

Associated with each model component is a free parameter that we estimate through out-of-sample cross validation. These parameters provide a window into distinct aspects of learning and exploration: 

1. The length-scale parameter $\lambda$ of the RBF kernel captures how strongly a participant generalizes based on the observed evidence, i.e., the rewards obtained from previous choices.
2. The uncertainty bonus $\beta$ represents to the level of directed exploration, i.e., how much expected rewards are inflated through an "uncertainty bonus".
3. The temperature parameter $\tau$ corresponds to the amount of sampling noise, i.e., extent of random exploration. 

## Model comparison

We tested the GP-UCB model in its ability to model learning and predicting each participants' search and decision-making behavior. To assess the contribution of each component of the model (generalization, uncertainty-directed exploration, and random exploration) we compare the predictive accuracy of the GP-UCB model to model variants where we lesion away each component.

$\lambda$


All models were fitted using leave-one-round-out cross-validation based on maximum likelihood estimation. Model fits are evaluated using the sum of negative log-likelihoods across all out-of-sample predictions.

Out-of-sample predictive accuracy was assessed using a pseudo-$R^2$ measure, based on the sum of negative log-likelihoods across all out-of-sample predictions. The summed negative log likelihoods are compared to a random model, such that $R^2=0$ corresponds to chance performance and $R^2=1$ corresponds to theoretically perfect predictions. 

$$
R^2 = 1 - \frac{\log \mathcal{L}(M_k)}{\log\mathcal{L}(M_{rand})},
$$



```{r}
#| fig-cap: "Predictive accuracy of GP-UCB model and lesioned variants."
#| label: fig_model_comparison

modelFits <- read.csv('data/modelFit.csv') %>% mutate(id = as.factor(id))
length(unique(modelFits$id))

groupDF <- dat %>% 
  group_by(id) %>%
  slice(1) %>%
  ungroup()

# length(unique(dat$id))
# length(unique(groupDF$id))

modelFits <- merge(modelFits, groupDF[,c('id', 'group')], by = "id") #merge to add group and condition data

modelFits <-  modelFits %>%
  mutate(kernel=factor(kernel, levels=c('RBF', 'BMT'), labels=c('GP', 'BMT'))) #%>%
# mutate(group = case_match(group,
#                           "PPD-" ~ "PD-",
#                           "PPD+" ~ "PD+",
#                           "PNP" ~ "PNP",
#                           .default = NA)) %>%
# mutate(group = factor(group, levels = c("PNP", "PD+", "PD-")))

modelFits$ModelName = paste(modelFits$kernel, modelFits$acq, sep="-")
modelFits$ModelName = factor(modelFits$ModelName, levels = c('GP-UCB', 'BMT-UCB', 'GP-GM', 'BMT-GM', 'GP-GV', 'BMT-GV'))
modelFits$acq <- factor(modelFits$acq, levels = c('UCB', 'GM', 'GV'))

#Only include key comparisons
modelFits <- subset(modelFits, ModelName %in% c('GP-UCB', 'GP-GM', 'BMT-UCB'))

#Two line name for models
modelFits$shortname <- factor(modelFits$ModelName, levels = c('GP-UCB','BMT-UCB', 'GP-GM'))
levels(modelFits$shortname) <- c('GP\nUCB', 'lambda\nlesion', 'beta\nlesion')

ggboxplot(modelFits, 
          x = "shortname", 
          y = "R2",
          color = "group", palette =groupcolors, fill = "group", alpha = 0.2,
          add = "jitter", jitter.size = 0.5, shape = "group", title = "Model comparison") +
  facet_wrap(~group, nrow = 1) +
  ylab(bquote(R^2)) +
  xlab("") +
  stat_compare_means(
    comparisons = list( 
      c("GP\nUCB", "lambda\nlesion"), 
      c("GP\nUCB", "beta\nlesion"), 
      c("lambda\nlesion", "beta\nlesion")  
    ), 
    paired = TRUE, 
    method = "t.test", 
    aes(label = paste0("p = ", after_stat(p.format)))
    
  ) +
  stat_summary(fun = mean, geom="point", shape = 23, fill = "white", size=2) +
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=12),
        legend.position = "none"
  )

ggsave("plots/model_comparison.png", width = 9, height = 5)
# p_R2_comp <- 
# ggplot(modelFits, aes(x=shortname, y=R2, fill=NA,color=shortname)) +
# #geom_line(aes(group=id), color = 'grey', alpha  = 0.3)+
# geom_quasirandom( size = 0.5)+
# geom_boxplot(width = 0.4, color ='black', outlier.shape=NA, fill = NA)+
# stat_summary(fun.y = mean, geom='point', shape = 23, color = 'black', fill = 'white')+
# xlab('') +
# ylab(expression(R^2)) +
# #ylab(expression(italic(pxp))) +
# scale_color_manual(values=modelPal, name = 'Model', labels = expression('GP-UCB', lambda*' lesion', beta* ' lesion')) +
# scale_fill_manual(values=modelPal, name = 'Model', labels = expression('GP-UCB', lambda*' lesion', beta* ' lesion')) +
# facet_wrap(~group, nrow = 1)+
# ggtitle('Model fits') +
# theme_classic() +
# theme(strip.background=element_blank(),
#       legend.position = 'none', legend.justification = c(1,0), axis.title.x=element_blank())

```


### Classification of participants according to max $R^2$

```{r}


df_participant_classification <- modelFits %>%
  group_by(id) %>%
  slice_max(order_by = R2, n = 1) %>%
  select(id, group, ModelName, shortname, R2) %>% 
  ungroup()

df_counts <- df_participant_classification %>%
  count(group, shortname)
```

We classified participants based on which model achieved the highest cross-validated predictive accuracy (highest $R^2$). Overall, `r sum(df_counts$n[df_counts$shortname == "GP\nUCB"])` out of `r sum(df_counts$n)` participants (`r round(sum(df_counts$n[df_counts$shortname == "GP\nUCB"]) / sum(df_counts$n),3)*100`%) were best described by the GP-UCB model, `r sum(df_counts$n[df_counts$shortname == "lambda\nlesion"])` by the lambda lesion model (`r round(sum(df_counts$n[df_counts$shortname == "lambda\nlesion"]) / sum(df_counts$n),3)*100`%) , `r sum(df_counts$n[df_counts$shortname == "beta\nlesion"])` by the beta lesion model (`r round(sum(df_counts$n[df_counts$shortname == "beta\nlesion"]) / sum(df_counts$n),3)*100`%), and `r sum(df_counts$n[df_counts$shortname == "tau\nlesion"])` by the tau lesion model (`r round(sum(df_counts$n[df_counts$shortname == "tau\nUCB"]) / sum(df_counts$n),3)*100`%) .

```{r}
#| fig-cap: "Predictive accuracy of GP-UCB model and lesioned variants."
#| label: fig_articipant_classification
#| fig-width: 7
#| fig-height: 7

ggplot(df_participant_classification, aes(x = shortname, fill = group)) +
  facet_wrap(~group) +
  geom_dotplot(method = 'histodot', binwidth = 1, dotsize = 0.6) +
  geom_text(data = df_counts, aes(x = shortname, y = n + 0.5, label = n), size = 5, color = "black") +
  scale_fill_manual(values = groupcolors) +
  scale_x_discrete("") + 
  scale_y_continuous(NULL, breaks = NULL) + 
  # guides(fill = "none") +
  # coord_fixed(0.9,ylim = c(0,15)) +
  coord_cartesian(ylim=c(0,13), expand = FALSE) +
  theme_classic() +
  theme(
    legend.title = element_blank(),
    legend.position = 'none',
    strip.text = element_text(size=14),
    legend.text =  element_text(colour="black"),
    text = element_text(colour = "black"),
    strip.background =element_blank(),
    axis.text.x = element_text(colour="black", size = 14),
    axis.text.y = element_text(colour="black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank())

ggsave("plots/participant_classification.png", width = 8, height = 8, dpi=300)
```


## Analysis of parameter estimates


```{r}
#| fig-cap: "Parameter estimates of GP-UCB model, estimated through leave-one-round-out cross validation. Each dot is one participant."
#| label: fig_GP-CUB_params

df_gpucb_params <- modelFits %>% filter(kernel=='GP' & acq == 'UCB') %>% 
  pivot_longer(c('lambda', 'beta', 'tau'), names_to = 'param', values_to = 'estimate') %>% 
  mutate(param = factor(param, levels = c('lambda', 'beta', 'tau'))) 

ggboxplot(df_gpucb_params, 
          x = "group", 
          y = "estimate",
          color = "group", palette =groupcolors, fill = "group", alpha = 0.2,
          add = "jitter", jitter.size = 0.5, shape = "group", title = "GP-UCB parameter estimates") +
  facet_wrap(~param, nrow = 1) +
  scale_y_log10(breaks = c(0.01, 0.1, 1, 10, 100), labels = c("0.01", "0.1", "1", "10", "100")) +
  ylab("Estimate") +
  xlab("") +
  stat_compare_means(comparisons = list( c("PNP", "PD+"), c("PD+", "PD-"), c("PNP", "PD-")  ), 
                     paired = F, 
                     method = "t.test", 
                     # label = "p.format",
                     aes(label = paste0("p = ", after_stat(p.format)))
  ) +
  stat_summary(fun = mean, geom="point", shape = 23, fill = "white", size=2) +
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=12),
        legend.position = "none"
  )

ggsave("plots/GP-UCB_params.png", width = 9, height = 5)
```

The increased exploration bonus $\beta$ aligns with findings from a restless bandit paradigm, where L-Dopa reduced the amount of directed exploration in healthy volunteers, while the level of undirected exploration was unaffected @Chakroun2020restless_ldopa.

### Correlation of Parameters with Performance

```{r}
#| fig-cap: "Correlation of GP-UCB parameters with obtained mean reward across all trials and rounds. Each dot is one participant. The insets show the correlations for a restricted parameter range from 0 to 1. "
#| label: fig_params_reward_cor
#| fig-width: 12
#| fig-height: 5

# mean reward per subject across all trials and rounds (practice and bonus round excluded)
df_mean_reward_subject <- dat %>% 
  filter(trial != 0 & round %in% 2:9) %>% # exclude first (randomly revealed) tile and practice round and bonus round
  group_by(id) %>% 
  summarise(group = first(group),
            sum_reward = sum(z),
            mean_reward = mean(z), 
            sd_reward = sd(z)) 

df_params_performance <- df_gpucb_params %>% 
  pivot_wider(names_from = param, values_from = estimate ) %>% 
  mutate(id = as.factor(id)) %>% 
  left_join(df_mean_reward_subject, by = "id")

df_params_performance <- df_gpucb_params %>% 
  #pivot_wider(names_from = param, values_from = estimate ) %>% 
  mutate(id = as.factor(id)) %>% 
  left_join(df_mean_reward_subject, by = "id") 


# p_GP_UCB_params_cor_reward <-  ggscatter(df_params_performance, x = "estimate", y = "mean_reward",
#    add = "reg.line",  
#    add.params = list(color = "darkred", fill = "lightgray"), 
#    conf.int = TRUE 
#    ) +
#    facet_wrap(~param, scales = "free_x") +
#    #stat_cor(method = "kendall", label.x = c(0,3,3), label.y = 45) +
#    scale_x_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100), labels = c("0.001", "0.01", "0.1", "1", "10", "100")) +
#    ggtitle("Correlation between GP-UCB parameters and reward") +
#   theme_classic() +
#   theme(strip.background = element_blank(),  
#         strip.text = element_text(color = "black", size=12),
#         legend.title = element_blank())
#  
#  ggsave("plots/GP-UCB_params_cor_reward.png", p_GP_UCB_params_cor_reward, width = 12, height= 4, dpi=300)


# plot correlation between parameter estimates and mean reward, with inset for smaller estimate range (ie. removal of outliers to test whether corrleation remains stable)
main_plot <- ggscatter(df_params_performance, x = "estimate", y = "mean_reward",
                       add = "reg.line",  
                       add.params = list(color = "darkred", fill = "lightgray"), 
                       conf.int = TRUE 
) +
  facet_wrap(~param, scales = "free_x") +
  stat_cor(method = "pearson", label.x = c(0,3,3), label.y = 45) +
  ggtitle("Correlation between GP-UCB parameters and reward") +
  scale_y_continuous("Mean reward", breaks = seq(0,45,10)) +
  xlab("Estimate") +
  theme_classic() +
  theme(strip.background = element_blank(),  
        strip.text = element_text(color = "black", size=12),
        legend.title = element_blank())

# Inset-Plot für beta (nur Werte 0-1)
inset_beta <- 
  ggscatter(df_params_performance %>% filter(param == "beta" & estimate > 0 & estimate <= 1), 
            x = "estimate", y = "mean_reward",
            add = "reg.line",  
            add.params = list(color = "darkred", fill = "lightgray"), 
            conf.int = TRUE 
  ) +
  stat_cor(method = "pearson", label.x = 0.1, label.y = 42, size = 3) +
  scale_x_continuous(breaks = seq(0,1,0.1)) +
  theme_classic() +
  theme(axis.title = element_blank(),strip.background = element_blank(), strip.text = element_blank(), legend.position = "none")

# Inset-Plot für tau (nur Werte 0-1)
inset_tau <- 
  ggscatter(df_params_performance %>% filter(param == "tau" & estimate > 0 & estimate <= 1), 
            x = "estimate", y = "mean_reward",
            add = "reg.line",  
            add.params = list(color = "darkred", fill = "lightgray"), 
            conf.int = TRUE 
  ) +
  stat_cor(method = "pearson", label.x = 0.1, label.y = 42, size = 3) +
  scale_x_continuous(breaks = seq(0,1,0.1)) +
  theme_classic() +
  theme(axis.title = element_blank(),strip.background = element_blank(), strip.text = element_blank(), legend.position = "none")


# Hauptplot mit Insets für beta und tau
p_GP_UCB_params_cor_reward_inset <- 
  ggdraw(main_plot) +
  draw_plot(inset_beta, x = 0.5, y = 0.5, width = 0.15, height = 0.3) +
  draw_plot(inset_tau, x = 0.8, y = 0.5, width = 0.15, height = 0.3)

p_GP_UCB_params_cor_reward_inset

ggsave("plots/GP-UCB_params_cor_reward.png", p_GP_UCB_params_cor_reward_inset, width = 12, height= 4, dpi=300)

```


# Session Information

<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne">

Session Information

</button>

:::: {#collapseOne .accordion-collapse .collapse}
<div>

```{r}
sessionInfo()
```

</div>
::::